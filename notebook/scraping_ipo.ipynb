{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45049,
     "status": "ok",
     "timestamp": 1649765112153,
     "user": {
      "displayName": "樅山輝",
      "userId": "13613188553709245558"
     },
     "user_tz": -540
    },
    "id": "bTDZnOe4E1sj",
    "outputId": "0a2da34c-1ba5-463f-93bf-d13d0085f8b3"
   },
   "outputs": [],
   "source": [
    "#初期設定\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import lxml\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs4\n",
    "import tqdm\n",
    "import sys\n",
    "import inspect\n",
    "# chromeのwebdriver自動更新用\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://web-kiwami.com/python-beautyfulsoup4.html\n",
    "# http://kondou.com/BS4/\n",
    "# bs4参考"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_year\n",
    "this_year = 2022 #年変わったら変更\n",
    "get_year = list(range(2011,this_year+1))\n",
    "df_result = pd.DataFrame()\n",
    "dict1 = {}\n",
    "for year in get_year:\n",
    "    if 2011 <= year <= 2017:\n",
    "        print(year, \"出力中\")\n",
    "        # 単年のデータ取得2011~2017\n",
    "        url =  rf\"https://www.ipokiso.com/company/{year}.html\"\n",
    "        html_res = requests.get(url)\n",
    "        # ページアクセスエラーの出力\n",
    "        if html_res.status_code != 200:\n",
    "            print(\"requests.getでのurlのアクセスができていません\")\n",
    "            lineno = inspect.currentframe().f_lineno\n",
    "            print(f\"エラーが発生しました。行番号: {lineno}\")\n",
    "            sys.exit()\n",
    "        soup = bs4(html_res.content, 'html.parser')\n",
    "        find_all_list = soup.find_all(href=re.compile(\"company/[0-9]{4}/\"))\n",
    "        url_dict = {\n",
    "            find_all_list[i].text:\"https://www.ipokiso.com/\" + find_all_list[i][\"href\"]\n",
    "            for i in range(len(find_all_list))\n",
    "        }\n",
    "        dict1.update(url_dict)\n",
    "        dfs_list = pd.read_html(url)\n",
    "        time.sleep(np.random.randint(100,120)/100)\n",
    "        for i in range(len(dfs_list)):\n",
    "            dfs_list[i][\"上場年\"] = f\"{year}\"\n",
    "            df_result = pd.concat([df_result, dfs_list[i]])\n",
    "            df_result = df_result.reset_index(drop=True)\n",
    "        # df_result.drop(df_result[df_result['企業名'] == \"企業名\"].index , inplace=True)\n",
    "        # df_result.drop(df_result[df_result['初値'] == \"初値\"].index , inplace=True)\n",
    "    else:\n",
    "        # 単年のデータ取得2018~2022\n",
    "        print(year, \"出力中\")\n",
    "        url = rf\"https://www.ipokiso.com/company/{year}.html\"\n",
    "        if year == 2022:\n",
    "            url = r\"https://www.ipokiso.com/company/index.html\"\n",
    "        html_res = requests.get(url)\n",
    "        # ページアクセスエラーの出力\n",
    "        if html_res.status_code != 200:\n",
    "            print(\"requests.getでのurlのアクセスができていません\")\n",
    "            lineno = inspect.currentframe().f_lineno\n",
    "            print(f\"エラーが発生しました。行番号: {lineno}\")\n",
    "            sys.exit()\n",
    "        soup = bs4(html_res.content, 'html.parser')\n",
    "        find_all_list = soup.find_all(href=re.compile(\"company/[0-9]{4}/\"))\n",
    "        url_dict = {\n",
    "            find_all_list[i].text:\"https://www.ipokiso.com/\" + find_all_list[i][\"href\"]\n",
    "            for i in range(len(find_all_list))\n",
    "        }\n",
    "        dict1.update(url_dict)\n",
    "        dfs_list = pd.read_html(url)\n",
    "        # サイト更新時にページのtableのデザインが変更していないかチェック\n",
    "        if year >= 2022:\n",
    "            try:\n",
    "                if dfs_list[0].columns[0] != '企業名':\n",
    "                    raise ValueError(\"銘柄一覧から取得したtableの値が「企業名」ではありません\")\n",
    "                elif dfs_list[1].columns[0] != '総合評価':\n",
    "                    raise ValueError(\"銘柄一覧から取得したtableの値が「総合評価」ではありません\")\n",
    "            except ValueError as e:\n",
    "                # エラーが発生した場合の処理\n",
    "                print(\"Error: {}\".format(e))\n",
    "                lineno = inspect.currentframe().f_lineno\n",
    "                print(f\"エラーが発生した行番号: {lineno}\")   \n",
    "        time.sleep(np.random.randint(100,120)/100)\n",
    "        for i in range(0, len(dfs_list) , 2):\n",
    "            dfs_list_con = pd.concat([dfs_list[i], dfs_list[i+1]], axis=1)\n",
    "            dfs_list_con[\"上場年\"] = f\"{year}\"\n",
    "            if year == 2019:\n",
    "                dfs_list_con = dfs_list_con.rename(columns={'上場 市場': '上場市場'})\n",
    "            if year >= 2020:\n",
    "                dfs_list_con = dfs_list_con.rename(columns={'申し込み期間': '申し込み 期間', '初値上昇率': '初値 上昇率'})\n",
    "            df_result = pd.concat([df_result, dfs_list_con],ignore_index=True)\n",
    "            df_result = df_result.reset_index(drop=True)\n",
    "        df_result.drop(df_result[df_result['企業名'] == \"企業名\"].index , inplace=True)\n",
    "        df_result.drop(df_result[df_result['初値'] == \"初値\"].index , inplace=True)\n",
    "    if year != this_year:\n",
    "        time.sleep(np.random.randint(100,120)/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict作成\n",
    "# 空白削除\n",
    "del dict1[\" \"]\n",
    "del dict1[\"\"]\n",
    "del dict1[\"\\n\"]\n",
    "# dictをdf化\n",
    "df_dict = pd.DataFrame.from_dict(dict1, orient='index')\n",
    "# カラム名変更\n",
    "df_dict = df_dict.rename(columns={0:\"url\"})\n",
    "# 間違えているurlを変更\n",
    "df_dict.url[df_dict.url == \"https://www.ipokiso.com/https://www.ipokiso.com/company/2013/zigexn.html\"] = \"https://www.ipokiso.com/company/2013/zigexn.html\"\n",
    "df_dict = df_dict.rename_axis('index').reset_index()\n",
    "df_dict = df_dict.rename(columns={'index':'company_name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 時短のためcode含めたcsv読み込み　コード完成後に削除する\n",
    "df_result2 = pd.read_csv(r\"C:\\Users\\xxp2p\\OneDrive\\デスクトップ\\df_dict.csv\", index_col=0)\n",
    "df_result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests.getでのデータ取得用ループ　評価\n",
    "def make_company_info(url):\n",
    "    html_res = requests.get(url)\n",
    "    if html_res.status_code != 200:\n",
    "        print(\"requests.getでのurlのアクセスができていません\")\n",
    "        lineno = inspect.currentframe().f_lineno\n",
    "        print(f\"エラーが発生しました。行番号: {lineno}\")\n",
    "        sys.exit()\n",
    "    time.sleep(np.random.randint(100,120)/100)\n",
    "    soup = bs4(html_res.content, 'html.parser')\n",
    "    # 全企業codeの取得\n",
    "    try:\n",
    "        code = re.search(r\"[0-9]{4}\", soup.title.text).group()\n",
    "    except AttributeError:\n",
    "        f = soup.find_all('h1',text=re.compile(r\"[0-9]{4}\"))[0].text\n",
    "        code = re.search(r\"[0-9]{4}\", f).group()\n",
    "    code_list.append(code)\n",
    "    # 成長性等の評価取得\n",
    "    table = soup.find('table',class_=\"company01\")\n",
    "    val = table.find_all('td')\n",
    "    # valからカラムの値に入れたい◎等の値のみ抽出\n",
    "    try:\n",
    "        val_list = []\n",
    "        for i in range(4):\n",
    "            if \">？<\" in str(val[i]):\n",
    "                keyword = \"？\"\n",
    "            elif \"/sannkaku02.gif\" in str(val[i]):\n",
    "                keyword = \"△\"\n",
    "            elif \"/sannkaku.gif\" in str(val[i]):\n",
    "                keyword = \"△\"\n",
    "            elif \"/sankaku.gif\" in str(val[i]):\n",
    "                keyword = \"△\"\n",
    "            elif \"/maru02.gif\" in str(val[i]):\n",
    "                keyword = \"〇\"\n",
    "            elif \"/maru.gif\" in str(val[i]):\n",
    "                keyword = \"〇\"\n",
    "            elif \"/2maru.gif\" in str(val[i]):\n",
    "                keyword = \"◎\"\n",
    "            elif \"/s.gif\" in str(val[i]):\n",
    "                keyword = \"S\"\n",
    "            elif \"/a.gif\" in str(val[i]):\n",
    "                keyword = \"A\"\n",
    "            elif \"/b.gif\" in str(val[i]):\n",
    "                keyword = \"B\"\n",
    "            elif \"/c.gif\" in str(val[i]):\n",
    "                keyword = \"C\"\n",
    "            elif \"/d.gif\" in str(val[i]):\n",
    "                keyword = \"D\"\n",
    "            val_list.append(keyword)\n",
    "    except AttributeError:\n",
    "        val_list = [np.nan,np.nan,np.nan,np.nan]\n",
    "        print(f\"成長性listのエラー{code}\")\n",
    "    evaluation_list.append(val_list)\n",
    "    print(code)\n",
    "code_list = []\n",
    "url_list = df_dict.url.values\n",
    "evaluation_list = []\n",
    "[make_company_info(url) for url in url_list[0:3]] #ここで検証のurl数変更！\n",
    "df_company_01 = pd.DataFrame(evaluation_list,columns=[\"成長性\",\"割安性\",\"話題性\",\"総合評価\"])\n",
    "df_result2[df_company_01.columns] = df_company_01\n",
    "print(df_result2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここ以下で作成　12/13～"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初値予想アンケートの取得実験用　12/13～　これをもとに追加、その後削除する\n",
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(),chrome_options=options)\n",
    "url =  rf\"https://www.ipokiso.com/company/2022/beex.html\"\n",
    "driver.get(url)\n",
    "html_res = driver.page_source.encode('utf-8')\n",
    "soup = bs4(html_res, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アンケート価格リスト\n",
    "soup.find_all(\"p\",class_=\"vote-left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アンケートの票リスト\n",
    "soup.find_all(\"div\",class_=\"vote-bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soup.find_all(\"div\",class_=\"vote-bar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_values_list = soup.find_all(\"p\",class_=\"vote-left\")\n",
    "number_of_votes_list = soup.find_all(\"div\",class_=\"vote-bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes_list = []\n",
    "for value in number_of_votes_list:\n",
    "    # print(value)\n",
    "    value = int(re.search(r\"\\\"[0-9]+\\\"\",str(value)).group().strip('\"'))\n",
    "    votes_list.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20221214編集中 アンケート情報取得する\n",
    "# seleniumでのデータ取得用ループ　評価\n",
    "# chromeのwebdriverのinstallしブラウザを起動する \n",
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(),chrome_options=options)\n",
    "def make_company_info(url):\n",
    "    driver.get(url)\n",
    "    #seleniumの場合ここでsleepしないと正しく取得出来ない場合がある\n",
    "    time.sleep(np.random.randint(100,120)/100)\n",
    "    # HTMLを文字コードをUTF-8に変換してから取得します。\n",
    "    html_res = driver.page_source.encode('utf-8')\n",
    "#ここでシステムエラー処理する？\n",
    "    # if html_res.status_code != 200:\n",
    "    #     print(\"requests.getでのurlのアクセスができていません\")\n",
    "    #     lineno = inspect.currentframe().f_lineno\n",
    "    #     print(f\"エラーが発生しました。行番号: {lineno}\")\n",
    "    #     sys.exit()\n",
    "    soup = bs4(html_res, 'html.parser')\n",
    "    # 全企業codeの取得\n",
    "    try:\n",
    "        code = re.search(r\"[0-9]{4}\", soup.title.text).group()\n",
    "    except AttributeError:\n",
    "        f = soup.find_all('h1',text=re.compile(r\"[0-9]{4}\"))[0].text\n",
    "        code = re.search(r\"[0-9]{4}\", f).group()\n",
    "    code_list.append(code)\n",
    "    # 成長性等の評価取得\n",
    "    table = soup.find('table',class_=\"company01\")\n",
    "    val = table.find_all('td')\n",
    "    # valからカラムの値に入れたい◎等の値のみ抽出\n",
    "    try:\n",
    "        val_list = []\n",
    "        for i in range(4):\n",
    "            if \">？<\" in str(val[i]):\n",
    "                keyword = \"？\"\n",
    "            elif \"/sannkaku02.gif\" in str(val[i]):\n",
    "                keyword = \"△\"\n",
    "            elif \"/sannkaku.gif\" in str(val[i]):\n",
    "                keyword = \"△\"\n",
    "            elif \"/sankaku.gif\" in str(val[i]):\n",
    "                keyword = \"△\"\n",
    "            elif \"/maru02.gif\" in str(val[i]):\n",
    "                keyword = \"〇\"\n",
    "            elif \"/maru.gif\" in str(val[i]):\n",
    "                keyword = \"〇\"\n",
    "            elif \"/2maru.gif\" in str(val[i]):\n",
    "                keyword = \"◎\"\n",
    "            elif \"/s.gif\" in str(val[i]):\n",
    "                keyword = \"S\"\n",
    "            elif \"/a.gif\" in str(val[i]):\n",
    "                keyword = \"A\"\n",
    "            elif \"/b.gif\" in str(val[i]):\n",
    "                keyword = \"B\"\n",
    "            elif \"/c.gif\" in str(val[i]):\n",
    "                keyword = \"C\"\n",
    "            elif \"/d.gif\" in str(val[i]):\n",
    "                keyword = \"D\"\n",
    "            val_list.append(keyword)\n",
    "    except AttributeError:\n",
    "        val_list = [np.nan,np.nan,np.nan,np.nan]\n",
    "        print(f\"成長性listのエラー{code}\")\n",
    "    print(code)\n",
    "    # アンケート調査結果の取得\n",
    "    print(len(soup.find_all(\"div\",class_=\"vote-bar\")))\n",
    "    # 6行の場合\n",
    "    if len(soup.find_all(\"div\",class_=\"vote-bar\")) == 6:\n",
    "        vote_list = []\n",
    "        number_of_votes_list = soup.find_all(\"div\",class_=\"vote-bar\")\n",
    "        for value in number_of_votes_list:\n",
    "            value = int(re.search(r\"\\\"[0-9]+\\\"\",str(value)).group().strip('\"'))\n",
    "            val_list.append(value)\n",
    "    # 5行の場合\n",
    "    elif len(soup.find_all(\"div\",class_=\"vote-bar\")) == 5:\n",
    "        vote_list = []\n",
    "        val_list.append(np.nan)\n",
    "        number_of_votes_list = soup.find_all(\"div\",class_=\"vote-bar\")\n",
    "        for value in number_of_votes_list:\n",
    "            value = int(re.search(r\"\\\"[0-9]+\\\"\",str(value)).group().strip('\"'))\n",
    "            val_list.append(value)\n",
    "    # 10行の場合 5行のアンケートがなぜか２つ表示されている銘柄\n",
    "    elif len(soup.find_all(\"div\",class_=\"vote-bar\")) == 10:\n",
    "        vote_list = []\n",
    "        val_list.append(np.nan)\n",
    "        number_of_votes_list = soup.find_all(\"div\",class_=\"vote-bar\")\n",
    "        number_of_votes_list = number_of_votes_list[0:5]\n",
    "        for value in number_of_votes_list:\n",
    "            value = int(re.search(r\"\\\"[0-9]+\\\"\",str(value)).group().strip('\"'))\n",
    "            val_list.append(value)\n",
    "    # アンケートがない場合\n",
    "    elif len(soup.find_all(\"div\",class_=\"vote-bar\")) == 0:\n",
    "        val_list.extend([np.nan,np.nan,np.nan,np.nan,np.nan,np.nan])\n",
    "    # その他の行数エラー検知\n",
    "    else:\n",
    "        print(\"票の価格帯の数が分岐にありません\")\n",
    "        driver.quit()\n",
    "        raise Exception(\"Error: アンケートの価格帯の数がifの分岐にありません\")\n",
    "    # 今まで取得した情報をリストに格納\n",
    "    evaluation_list.append(val_list)\n",
    "code_list = []\n",
    "url_list = df_dict.url.values\n",
    "evaluation_list = []\n",
    "[make_company_info(url) for url in url_list[0:]] #ここで検証のurl数変更！\n",
    "print(evaluation_list)\n",
    "df_company_01 = pd.DataFrame(evaluation_list,columns=[\n",
    "    \"成長性\",\n",
    "    \"割安性\",\n",
    "    \"話題性\",\n",
    "    \"総合評価\",\n",
    "    \"初値予想_+200%以上\",\n",
    "    \"初値予想_+100%以上+200%未満\",\n",
    "    \"初値予想_+50%以上+100%未満\",\n",
    "    \"初値予想_+20%以上+50%未満\",\n",
    "    \"初値予想_+0%以上+20%未満\",\n",
    "    \"初値予想_+0%未満\",\n",
    "])\n",
    "df_result2[df_company_01.columns] = df_company_01\n",
    "print(df_result2)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result2.to_csv(r\"C:\\Users\\xxp2p\\OneDrive\\デスクトップ\\df_result3.csv\", encoding='utf-8_sig',index=False)\n",
    "\n",
    "df_result2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これを元に作成2022/12/09~ ↓　全銘柄チェック、その後、例外の銘柄のtableの変更実施\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本番用\n",
    "def make_company_info_2(url):\n",
    "    dfs_table_list = pd.read_html(url)\n",
    "    print(dfs_table_list[1].iloc[0,1])\n",
    "    time.sleep(np.random.randint(100,120)/100)\n",
    "    # サイト更新時にデザイン変更がないか検知する（現状ワシントンホテルのみ違うためスルー）\n",
    "    # 回避urlリスト\n",
    "    avoidance_url_list = [\n",
    "        \"https://www.ipokiso.com//company/2019/washingtonhotel.html\",#[0]に１つ余分に優待情報のtableある\n",
    "        \"https://www.ipokiso.com//company/2020/gmo-fg.html\",#table[6][7]が[7][8]にずれている\n",
    "        \"https://www.ipokiso.com//company/2021/frontier.html\",#table[6]が無い\n",
    "        \"https://www.ipokiso.com//company/2021/geolocation.html\",#table[6]が無い\n",
    "        ]\n",
    "    if url not in avoidance_url_list:\n",
    "        try:\n",
    "            if dfs_table_list[1].iloc[0,0] != \"会社名\":\n",
    "                raise ValueError(\"銘柄一覧から取得したtableの値が「会社名」ではありません\")\n",
    "            elif dfs_table_list[2].iloc[0,0] != '想定価格':\n",
    "                raise ValueError(\"銘柄一覧から取得したtableの値が「想定価格」ではありません\")               \n",
    "            elif dfs_table_list[3].iloc[0,0] != '抽選申込期間':\n",
    "                raise ValueError(\"銘柄一覧から取得したtableの値が「抽選申込期間」ではありません\")\n",
    "            elif not '公募' in dfs_table_list[4].iloc[0,0]:\n",
    "                raise ValueError(\"銘柄一覧から取得したtableの値が「公募株数等」ではありません\")\n",
    "            elif dfs_table_list[5].iloc[0,1] != '証券会社名':\n",
    "                raise ValueError(\"銘柄一覧から取得したtableの値が「証券会社名」ではありません\")\n",
    "        except ValueError as e:\n",
    "            # エラーが発生した場合の処理\n",
    "            print(\"Error: {}\".format(e))\n",
    "            lineno = inspect.currentframe().f_lineno\n",
    "            print(f\"エラーが発生した行番号: {lineno}\")\n",
    "            print(f\"エラーが発生したURL:{url}\")\n",
    "            print(dfs_table_list)\n",
    "            # sys.exit()\n",
    "        # 会社名にREITと投資法人という文字列が無いことを判定する\n",
    "        if not (\"REIT\" in dfs_table_list[1].iloc[0,1]) and not (\"投資法人\" in dfs_table_list[1].iloc[0,1]):\n",
    "            try:\n",
    "                if dfs_table_list[6].columns[0] != '株主名':\n",
    "                    raise ValueError(\"銘柄一覧から取得したtableの値が「株主名」ではありません\")\n",
    "                elif not (\"（百万円）\" in dfs_table_list[7].iloc[1,0]) and not (\"（千米ドル）\" in dfs_table_list[7].iloc[1,0]):\n",
    "                    raise ValueError(\"銘柄一覧から取得したtableの値が「財務データ等」ではありません\")\n",
    "            except ValueError as e:\n",
    "                # エラーが発生した場合の処理\n",
    "                print(\"Error: {}\".format(e))\n",
    "                lineno = inspect.currentframe().f_lineno\n",
    "                print(f\"エラーが発生した行番号: {lineno}\")\n",
    "                print(f\"エラーが発生したURL:{url}\")\n",
    "                print(dfs_table_list)\n",
    "                # sys.exit()\n",
    "\n",
    "    # ここからdfsの結合、整形コード\n",
    "    df_con_list = []\n",
    "    for i in range(len(dfs_table_list)):\n",
    "        # 基本情報\n",
    "        if i == 1:\n",
    "            # tableの位置指定\n",
    "            df_table = dfs_table_list[i]\n",
    "            # table位置が違うものの例外処理\n",
    "            if df_table.iloc[0,0] != \"会社名\":\n",
    "                df_table = dfs_table_list[i + 1]\n",
    "                print(url)\n",
    "            # df整形：基本情報\n",
    "            cols = df_table.T.values[0]\n",
    "            val = df_table.T.values[1]\n",
    "            # 追加する1件分の基本情報のdf作成\n",
    "            df_table_1 = pd.DataFrame([val],columns=cols)\n",
    "            df_con_list.append(df_table_1)\n",
    "            # 表記の揺れ統一のための修正\n",
    "\n",
    "        # IPO日程と価格決定（初値予想）\n",
    "        if i == 2:\n",
    "            # tableの位置指定\n",
    "            df_table = dfs_table_list[i]\n",
    "            # table位置が違うものの例外処理\n",
    "            if dfs_table_list[2].iloc[0,0] != '想定価格':\n",
    "                df_table = dfs_table_list[i + 1]\n",
    "                print(url)\n",
    "            # df整形：IPO日程と価格決定（初値予想）\n",
    "            cols = df_table.T.values[0]\n",
    "            val = df_table.T.values[1]\n",
    "            df_table_2 = pd.DataFrame([val],columns=cols)\n",
    "            df_con_list.append(df_table_2)\n",
    "            # 表記の揺れ統一のための修正\n",
    "            \n",
    "        # IPOスケジュール\n",
    "        if i == 3:\n",
    "            # tableの位置指定 # tableの位置指定\n",
    "            df_table = dfs_table_list[i]\n",
    "             # table位置が違うものの例外処理\n",
    "            if dfs_table_list[3].iloc[0,0] != '抽選申込期間':\n",
    "                df_table = dfs_table_list[i + 1]\n",
    "                print(url)\n",
    "            \n",
    "            # df整形：IPOスケジュール\n",
    "            cols = df_table.T.values[0]\n",
    "            val = df_table.T.values[1]\n",
    "            df_table_3 = pd.DataFrame([val],columns=cols)\n",
    "            df_con_list.append(df_table_3)\n",
    "            # 表記の揺れ統一のための修正\n",
    "        # IPO当選株数\n",
    "        if i == 4:\n",
    "            # tableの位置指定\n",
    "            df_table = dfs_table_list[i]\n",
    "             # table位置が違うものの例外処理\n",
    "            if not '公募' in dfs_table_list[4].iloc[0,0]:\n",
    "                df_table = dfs_table_list[i + 1]\n",
    "                print(url)\n",
    "            # ワシントンホテルのみ[1]に優待情報があるためtableをずらしている\n",
    "            if url == \"https://www.ipokiso.com//company/2019/washingtonhotel.html\":\n",
    "                df_table = dfs_table_list[i + 1]\n",
    "            \n",
    "            # df整形：IPO当選株数\n",
    "            cols = df_table.T.values[0]\n",
    "            val = df_table.T.values[1]\n",
    "            df_table_4 = pd.DataFrame([val],columns=cols)\n",
    "            df_con_list.append(df_table_4)\n",
    "            # 表記の揺れ統一のための修正\n",
    "\n",
    "        # 幹事証券リスト（管理人独自予想あり）\n",
    "        if i == 5:\n",
    "            # tableの位置指定\n",
    "            df_table = dfs_table_list[i]\n",
    "             # table位置が違うものの例外処理\n",
    "            if dfs_table_list[5].iloc[0,1] != '証券会社名':\n",
    "                df_table = dfs_table_list[i + 1]\n",
    "                print(url)\n",
    "            # df整形：幹事証券リスト（管理人独自予想あり）\n",
    "            df_table_5 = pd.DataFrame()\n",
    "            for j in range(1,len(df_table.iloc[:,0])):\n",
    "                val = df_table.iloc[j,:].values\n",
    "                cols = df_table.iloc[0,:]\n",
    "                cols[0] = \"幹事種類\"\n",
    "                cols = [f\"{col}_{j}\" for col in cols]\n",
    "                df_add = pd.DataFrame([val],columns=cols)\n",
    "                df_table_5 = pd.concat([df_table_5, df_add], axis=1)\n",
    "            df_con_list.append(df_table_5)\n",
    "\n",
    "        # 株主構成、ロックアップなど\n",
    "        if i == 6:\n",
    "            if dfs_table_list[1].iloc[0,0] == \"会社名\":\n",
    "                company_name = dfs_table_list[1].iloc[0,1]\n",
    "            elif dfs_table_list[2].iloc[0,0] == \"会社名\":\n",
    "                company_name = dfs_table_list[2].iloc[0,1]\n",
    "            if not (\"REIT\" in company_name or \"投資法人\" in company_name or  (\"福証\" in company_name and not \"東証\" in company_name)):\n",
    "                # tableの位置指定\n",
    "                df_table = dfs_table_list[i]\n",
    "                # table位置が違うものの例外処理\n",
    "                if dfs_table_list[6].columns[0] != '株主名':\n",
    "                    df_table = dfs_table_list[i + 1]\n",
    "                    print(url)\n",
    "                df_table_6 = pd.DataFrame()\n",
    "                for k in range(0,len(df_table.iloc[:,0])):\n",
    "                    val = df_table.iloc[k,:].values\n",
    "                    cols = df_table.columns\n",
    "                    cols = [f\"{col}_{k+1}\" for col in cols]\n",
    "                    df = pd.DataFrame([val],columns=cols)\n",
    "                    df_table_6 = pd.concat([df_table_6, df], axis=1)\n",
    "                df_con_list.append(df_table_6)\n",
    "\n",
    "            # 企業業績のデータ（5年分）\n",
    "        if i == 7:\n",
    "            if not (\"REIT\" in company_name or \"投資法人\" in company_name):\n",
    "\n",
    "                # tableの位置指定\n",
    "                df_table = dfs_table_list[i]\n",
    "                # table位置が違うものの例外処理\n",
    "                if not (\"（百万円）\" in dfs_table_list[7].iloc[1,0]) and not (\"（千米ドル）\" in dfs_table_list[7].iloc[1,0]):\n",
    "                    df_table = dfs_table_list[i + 1]\n",
    "                    print(url)\n",
    "                \n",
    "                df_table_7 = pd.DataFrame()\n",
    "                for h in range(1,len(df_table.T.iloc[:,0])):\n",
    "                    val = df_table.T.iloc[h,:].values\n",
    "                    # カラム名の誤字修正\n",
    "                    cols = df_table.T.iloc[0,:]\n",
    "                    cols[0] = \"年月\"\n",
    "                    cols = [f\"{col}_{h}年目\" for col in cols]\n",
    "                    df = pd.DataFrame([val],columns=cols)\n",
    "                    df_table_7 = pd.concat([df_table_7, df], axis=1)\n",
    "                df_con_list.append(df_table_7)\n",
    "\n",
    "\n",
    "            # 全件のdfに追加していく\n",
    "    df_table_all = pd.concat(df_con_list,axis=1)\n",
    "    print(url)\n",
    "    print(\"finish\")\n",
    "    return df_table_all\n",
    "df_all = pd.DataFrame()\n",
    "url_list = df_dict.url.values\n",
    "for url in url_list[0:]:#ここでurl数変更\n",
    "    df_table_all = make_company_info_2(url) \n",
    "    df_all = pd.concat([df_all,df_table_all],axis=0,ignore_index=True)\n",
    "print(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv(r\"C:\\Users\\xxp2p\\OneDrive\\デスクトップ\\df_all.csv\", encoding='utf-8_sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_copy = pd.read_csv(r\"C:\\Users\\xxp2p\\OneDrive\\デスクトップ\\df_all.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上本番用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "# csv変換 時短のためこれを読み込む\n",
    "df_code = pd.DataFrame(code_list,columns=[\"code\"])\n",
    "df_dict[df_code.columns] = df_code.values\n",
    "df_result2.to_csv(\"df_result2\", encoding='utf-8_sig')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 時短のためcode含めたcsv読み込み\n",
    "df_result2 = pd.read_csv(\"df_dict.csv\", index_col=0)\n",
    "df_result2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- IPO当選株数のデータ取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下データ成長性等取得する\n",
    "url = \"https://www.ipokiso.com//company/2019/washingtonhotel.html\"\n",
    "html_res = requests.get(url)\n",
    "time.sleep(np.random.randint(100,120)/100)\n",
    "soup = bs4(html_res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_table_list = pd.read_html(\"https://www.ipokiso.com//company/2016/ichigo_green.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"REIT\" in dfs_table_list[1].iloc[0,1]\n",
    "\"投資法人\" in dfs_table_list[1].iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\"REIT\" in dfs_table_list[1].iloc[0,1 ] or \"投資法人\" in dfs_table_list[1].iloc[0,1]):\n",
    "    print(\"hi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_html_table_2011[7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_html_table = pd.read_html(\"https://www.ipokiso.com/company/2022/tms-japan.html\")\n",
    "df_html_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0]の取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_company_01 = pd.DataFrame(evaluation_list,columns=[\"成長性\",\"割安性\",\"話題性\",\"総合評価\"])\n",
    "df_result2[df_company_01.columns] = df_company_01\n",
    "print(df_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code_listの作成（詳細情報の取得ループ 全てのcode取得[0]までok！\n",
    "def make_company_info(url):\n",
    "    html_res = requests.get(url)\n",
    "    if html_res.status_code != 200:\n",
    "        print(\"requests.getでのurlのアクセスができていません\")\n",
    "        lineno = inspect.currentframe().f_lineno\n",
    "        print(f\"エラーが発生しました。行番号: {lineno}\")\n",
    "        sys.exit()\n",
    "    time.sleep(np.random.randint(100,120)/100)\n",
    "    soup = bs4(html_res.content, 'html.parser')\n",
    "    # 全企業codeの取得\n",
    "    try:\n",
    "        code = re.search(r\"[0-9]{4}\", soup.title.text).group()\n",
    "    except AttributeError:\n",
    "        f = soup.find_all('h1',text=re.compile(r\"[0-9]{4}\"))[0].text\n",
    "        code = re.search(r\"[0-9]{4}\", f).group()\n",
    "    code_list.append(code)\n",
    "    # 成長性等の評価取得\n",
    "    table = soup.find('table',class_=\"company01\")\n",
    "    val = table.find_all('td')\n",
    "    # valからカラムの値に入れたい◎等の値のみ抽出\n",
    "    try:\n",
    "        val_list = []\n",
    "        for i in range(4):\n",
    "            if \">？<\" in str(val[i]):\n",
    "                keyword = \"？\"\n",
    "            elif \"/sannkaku02.gif\" in str(val[i]):\n",
    "                keyword = \"△\"\n",
    "            elif \"/sannkaku.gif\" in str(val[i]):\n",
    "                keyword = \"△\"\n",
    "            elif \"/sankaku.gif\" in str(val[i]):\n",
    "                keyword = \"△\"\n",
    "            elif \"/maru02.gif\" in str(val[i]):\n",
    "                keyword = \"〇\"\n",
    "            elif \"/maru.gif\" in str(val[i]):\n",
    "                keyword = \"〇\"\n",
    "            elif \"/2maru.gif\" in str(val[i]):\n",
    "                keyword = \"◎\"\n",
    "            elif \"/s.gif\" in str(val[i]):\n",
    "                keyword = \"S\"\n",
    "            elif \"/a.gif\" in str(val[i]):\n",
    "                keyword = \"A\"\n",
    "            elif \"/b.gif\" in str(val[i]):\n",
    "                keyword = \"B\"\n",
    "            elif \"/c.gif\" in str(val[i]):\n",
    "                keyword = \"C\"\n",
    "            elif \"/d.gif\" in str(val[i]):\n",
    "                keyword = \"D\"\n",
    "            else:\n",
    "                keyword = np.nan\n",
    "            val_list.append(keyword)\n",
    "    except AttributeError:\n",
    "        val_list = [np.nan,np.nan,np.nan,np.nan]\n",
    "        print(f\"成長性listのエラー{code}\")\n",
    "    evaluation_list.append(val_list)\n",
    "    print(code)\n",
    "code_list = []\n",
    "url_list = df_dict.url.values\n",
    "evaluation_list = []\n",
    "[make_company_info(url) for url in url_list[0:3]] #ここで検証のurl数変更！\n",
    "df_company_01 = pd.DataFrame(evaluation_list,columns=[\"成長性\",\"割安性\",\"話題性\",\"総合評価\"])\n",
    "df_result2[df_company_01.columns] = df_company_01\n",
    "print(df_result2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv(r\"C:\\Users\\xxp2p\\OneDrive\\デスクトップ\\df_all.csv\", encoding='utf-8_sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全table取得（pd_htmlによる）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_html_table[2].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_html_table[2].T.values[0]\n",
    "# 基本情報のval取得\n",
    "val = df_html_table[2].T.values[1]\n",
    "# 追加する1件分の基本情報のdf作成\n",
    "df_table_1 = pd.DataFrame([val],columns=cols)\n",
    "df_table_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_html_table[3].T.values[0]\n",
    "# 基本情報のval取得\n",
    "val = df_html_table[3].T.values[1]\n",
    "# 追加する1件分の基本情報のdf作成\n",
    "df_table_2 = pd.DataFrame([val],columns=cols)\n",
    "df_table_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_html_table[4].T.values[0]\n",
    "# 基本情報のval取得\n",
    "val = df_html_table[4].T.values[1]\n",
    "# 追加する1件分の基本情報のdf作成\n",
    "df_table_3 = pd.DataFrame([val],columns=cols)\n",
    "df_table_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_html_table[5].T.values[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_html_table[5].T.values[0]\n",
    "# 基本情報のval取得\n",
    "val = df_html_table[5].T.values[1]\n",
    "# 追加する1件分の基本情報のdf作成\n",
    "df_table_4 = pd.DataFrame([val],columns=cols)\n",
    "df_table_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df_table_1,df_table_2],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_all.empty == True:\n",
    "    print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_all,df_table_all],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table_all = pd.merge(df_table_1,df_table_2,on=[\"key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20221201 ループで全table、全銘柄取り出す。（細かいデータの整形は後でやる）\n",
    "def make_company_info(url):\n",
    "    df_html_table = pd.read_html(url)\n",
    "    for i in range(0,8):\n",
    "\n",
    "        # 基本情報\n",
    "        if i == 0:\n",
    "            df_table = df_html_table[i + 1]\n",
    "            # ワシントンホテルのみ[1]に優待情報があるためtableをずらしている\n",
    "            if url == \"https://www.ipokiso.com//company/2019/washingtonhotel.html\":\n",
    "                df_table = df_html_table[i + 2]\n",
    "            # 基本情報のcols取得\n",
    "            cols = df_table.T.values[0]\n",
    "            # 基本情報のval取得\n",
    "            val = df_table.T.values[1]\n",
    "            # 追加する1件分の基本情報のdf作成\n",
    "            df_table_1 = pd.DataFrame([val],columns=cols)\n",
    "        # IPO日程と価格決定（初値予想）\n",
    "        if i == 1:\n",
    "            df_table = df_html_table[i + 1]\n",
    "            # ワシントンホテルのみ[1]に優待情報があるためtableをずらしている\n",
    "            if url == \"https://www.ipokiso.com//company/2019/washingtonhotel.html\":\n",
    "                df_table = df_html_table[i + 2]\n",
    "            cols = df_table.T.values[0]\n",
    "            val = df_table.T.values[1]\n",
    "            df_table_2 = pd.DataFrame([val],columns=cols)\n",
    "        # 次ここから\n",
    "        # if i == 2:\n",
    "\n",
    "        # 全件のdfに追加していく\n",
    "    df_table_all = pd.concat([df_table_1,df_table_2],axis=1)\n",
    "    \n",
    "    return df_table_all\n",
    "df_all = pd.DataFrame()\n",
    "code_list = []\n",
    "url_list = df_dict.url.values\n",
    "evaluation_list = []\n",
    "for url in url_list[0:3]:\n",
    "    df_table_all = make_company_info(url)\n",
    "    df_all = pd.concat([df_all,df_table_all],axis=0)\n",
    "# df = [make_company_info(url) for url in url_list[0:3]] #ここで検証のurl数変更！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]の取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_html_table_2011 = pd.read_html(\"https://www.ipokiso.com//company/2019/washingtonhotel.html\")\n",
    "cols = df_html_table_2011.T.values[0]\n",
    "val = df_html_table_2011.T.values[1]\n",
    "df_basic_info2 = pd.DataFrame([val],columns=cols)\n",
    "df_basic_info2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_html_table_2011 = pd.read_html(\"https://www.ipokiso.com//company/2019/washingtonhotel.html\")[1]\n",
    "cols = df_html_table_2011.T.values[0]\n",
    "val = df_html_table_2011.T.values[1]\n",
    "df_basic_info = pd.DataFrame([val],columns=cols)\n",
    "df_basic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_html_table_2011 = pd.read_html(\"https://www.ipokiso.com//company/2019/washingtonhotel.html\")\n",
    "\n",
    "df_html_table_2011[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 時短のためcode含めたcsv読み込み\n",
    "df_result2 = pd.read_csv(\"df_dict.csv\", index_col=0)\n",
    "df_result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 202221130\n",
    "def make_company_info(url):\n",
    "    html_res = requests.get(url)\n",
    "    if html_res.status_code != 200:\n",
    "        print(\"requests.getでのurlのアクセスができていません\")\n",
    "        lineno = inspect.currentframe().f_lineno\n",
    "        print(f\"エラーが発生しました。行番号: {lineno}\")\n",
    "        sys.exit()\n",
    "    time.sleep(np.random.randint(100,120)/100)\n",
    "    soup = bs4(html_res.content, 'html.parser')\n",
    "    # 全企業codeの取得\n",
    "    try:\n",
    "        code = re.search(r\"[0-9]{4}\", soup.title.text).group()\n",
    "    except AttributeError:\n",
    "        f = soup.find_all('h1',text=re.compile(r\"[0-9]{4}\"))[0].text\n",
    "        code = re.search(r\"[0-9]{4}\", f).group()\n",
    "    code_list.append(code)\n",
    "    # table[0]成長性等の評価取得\n",
    "    table = soup.find('table',class_=\"company01\")\n",
    "    val = table.find_all('td')\n",
    "    # valからカラムの値に入れたい◎等の値のみ抽出\n",
    "    try:\n",
    "        val_list = []\n",
    "        for i in range(4):\n",
    "            if \">？<\" in str(val[i]):\n",
    "                keyword = \"？\"\n",
    "            elif \"/sannkaku02.gif\" in str(val[i]):\n",
    "                keyword = \"△\"\n",
    "            elif \"/sannkaku.gif\" in str(val[i]):\n",
    "                keyword = \"△\"\n",
    "            elif \"/sankaku.gif\" in str(val[i]):\n",
    "                keyword = \"△\"\n",
    "            elif \"/maru02.gif\" in str(val[i]):\n",
    "                keyword = \"〇\"\n",
    "            elif \"/maru.gif\" in str(val[i]):\n",
    "                keyword = \"〇\"\n",
    "            elif \"/2maru.gif\" in str(val[i]):\n",
    "                keyword = \"◎\"\n",
    "            elif \"/s.gif\" in str(val[i]):\n",
    "                keyword = \"S\"\n",
    "            elif \"/a.gif\" in str(val[i]):\n",
    "                keyword = \"A\"\n",
    "            elif \"/b.gif\" in str(val[i]):\n",
    "                keyword = \"B\"\n",
    "            elif \"/c.gif\" in str(val[i]):\n",
    "                keyword = \"C\"\n",
    "            elif \"/d.gif\" in str(val[i]):\n",
    "                keyword = \"D\"\n",
    "            else:\n",
    "                keyword = np.nan\n",
    "            val_list.append(keyword)\n",
    "    except AttributeError:\n",
    "        val_list = [np.nan,np.nan,np.nan,np.nan]\n",
    "        print(f\"成長性listのエラー{code}\")\n",
    "    evaluation_list.append(val_list)\n",
    "\n",
    "    # read_htmlでtable情報取得\n",
    "    df_html_table = pd.read_html(url)\n",
    "\n",
    "    #[1]基本情報\n",
    "    # 関数外の変数が取れないためglobalで定義する\n",
    "    global df_basic_info\n",
    "    time.sleep(np.random.randint(100,120)/100)\n",
    "    for i in range(1,8):\n",
    "        df_table = df_html_table[1 + i].copy()\n",
    "        # ワシントンホテルのみ[1]に優待情報があるためtableをずらしている\n",
    "        if url == \"https://www.ipokiso.com//company/2019/washingtonhotel.html\":\n",
    "            df_table = df_html_table[2 + i].copy()\n",
    "        # 基本情報のcols取得\n",
    "        cols = df_table.T.values[0]\n",
    "        # 基本情報のval取得\n",
    "        val = df_table.T.values[1]\n",
    "        # 追加する1件分の基本情報のdf作成\n",
    "        df_table_new = pd.DataFrame([val],columns=cols)\n",
    "        print(df_table_new)\n",
    "        # 全件のdfに追加していく\n",
    "        # df_basic_info = pd.merge([df_basic_info,df_table_new])\n",
    "        return df_basic_info\n",
    "\n",
    "    \"\"\"\n",
    "    # table基本情報のみ取取得\n",
    "    df_table_basic = df_html_table[1].copy()\n",
    "    # ワシントンホテルのみ[1]に優待情報があるためtableをずらしている\n",
    "    if url == \"https://www.ipokiso.com//company/2019/washingtonhotel.html\":\n",
    "        df_table_basic = df_html_table[2].copy()\n",
    "    # 基本情報のcols取得\n",
    "    cols = df_table_basic.T.values[0]\n",
    "    # 基本情報のval取得\n",
    "    val = df_table_basic.T.values[1]\n",
    "    # 追加する1件分の基本情報のdf作成\n",
    "    df_basic_info_new = pd.DataFrame([val],columns=cols)\n",
    "    # 全件のdfに追加していく\n",
    "    df_basic_info = pd.concat([df_basic_info,df_basic_info_new],ignore_index=True)\n",
    "    print(code)\n",
    "    print(evaluation_list)\n",
    "    return df_basic_info\n",
    "    \"\"\"\n",
    "    \n",
    "code_list = []\n",
    "url_list = df_dict.url.values\n",
    "evaluation_list = []\n",
    "\n",
    "\n",
    "# 基本情報の空df作成\n",
    "df_basic_info = pd.DataFrame([],columns=[\"会社名\",\"コード\",\"市場\",\"会社URL\",\"狙い目証券会社\",\"会社設立\"])\n",
    "[make_company_info(url) for url in url_list[range(3)]] #ここで検証のurl数変更！\n",
    "df_company_01 = pd.DataFrame(evaluation_list,columns=[\"成長性\",\"割安性\",\"話題性\",\"総合評価\"])\n",
    "df_result2[df_company_01.columns] = df_company_01\n",
    "#[1]データ分割、整理、結合　codeの場所を分ける？ \n",
    "# 結合(axis=1で横方向の結合)\n",
    "df_result2 = pd.concat([df_result2,df_basic_info],axis=1)\n",
    "\n",
    "\n",
    "print(df_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basic_info.to_csv(r\"C:\\Users\\xxp2p\\OneDrive\\デスクトップ\\df_basic_info.csv\", encoding='utf-8_sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2]の取得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3]の取得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4]の取得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5]の取得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6]の取得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[7]の取得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以降参考データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_html_table.T.iloc[1:,:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_html_table = pd.read_html(\"https://www.ipokiso.com/company/2022/tms-japan.html\")[4]\n",
    "\n",
    "cols = df_html_table.T.iloc[0,:].values\n",
    "val = df_html_table.T.iloc[1:,:].values.flatten()\n",
    "df_ipo_stock_info = pd.DataFrame([val],columns=cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.subで文字列置換している　(arr.split(\"株\")[i])の文字列のr'/D'に一致したものを''に置換する\n",
    "# \\Dは数字以外の文字列\n",
    "# splitで株で区切って株自体は消す。配列にいれる\n",
    "# intで数字に変換\n",
    "stock_info = [\n",
    "    int(re.sub( r'\\D', '', (arr.split(\"株\")[i]))) \n",
    "    for arr in val\n",
    "    for i in range(3) \n",
    "]\n",
    "# ravelは配列を一次元化する\n",
    "# ravel()は可能な限りビューを返すが、flatten()は常にコピーを返すという違いがある。\n",
    "# カラムを２つ国内と海外を作成してリスト化、1次元化している。\n",
    "cols_1 = np.ravel([[c,c+\"_国内\",c+\"_海外\"] for c in cols])\n",
    "# 分割したデータと、名称を変更したカラムでデータフレームを作成している。\n",
    "df_stock_info = pd.DataFrame([stock_info],columns=cols_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以後チェック用code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLdict（単年）作成 確認用\n",
    "year = \"2020\"\n",
    "url = f'https://www.ipokiso.com/company/{year}.html'\n",
    "html_res = requests.get(url)\n",
    "time.sleep(np.random.randint(100,120)/100)\n",
    "soup = bs4(html_res.content, 'html.parser')\n",
    "find_all_list = soup.find_all(href=re.compile(f\"company/{year}/\"))\n",
    "url_dict = {\n",
    "    find_all_list[i].text:\"https://www.ipokiso.com/\" + soup.find_all(href=re.compile(f\"company/{year}/\"))[i][\"href\"]\n",
    "    for i in range(len(find_all_list))\n",
    "}\n",
    "url_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単年のデータ取得2011~2017　確認用\n",
    "df_result = pd.DataFrame()\n",
    "year = \"2011\"\n",
    "url =  rf\"https://www.ipokiso.com/company/{year}.html\"\n",
    "html_res = requests.get(url)\n",
    "time.sleep(np.random.randint(100,120)/100)\n",
    "soup = bs4(html_res.content, 'html.parser')\n",
    "find_all_list = soup.find_all(href=re.compile(f\"company/{year}/\"))\n",
    "print(len(find_all_list))\n",
    "df_url = pd.read_html(url)\n",
    "time.sleep(np.random.randint(100,120)/100)\n",
    "for i in range(len(df_url)):\n",
    "    df_url[i][\"上場年\"] = f\"{year}\"\n",
    "    df_result = pd.concat([df_result, df_url[i]])\n",
    "    df_result = df_result.reset_index(drop=True)\n",
    "df_result.drop(df_result[df_result['企業名'] == \"企業名\"].index , inplace=True)\n",
    "df_result.drop(df_result[df_result['初値'] == \"初値\"].index , inplace=True)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単年のデータ取得2018~2022　確認用\n",
    "df_result = pd.DataFrame()\n",
    "year = \"2022\"\n",
    "url =  rf\"https://www.ipokiso.com/company/{year}.html\"\n",
    "if year == \"2022\":\n",
    "    url =  r\"https://www.ipokiso.com/company/index.html\"\n",
    "html_res = requests.get(url)\n",
    "time.sleep(np.random.randint(100,120)/100)\n",
    "soup = bs4(html_res.content, 'html.parser')\n",
    "find_all_list = soup.find_all(href=re.compile(f\"company/{year}/\"))\n",
    "print(len(find_all_list))\n",
    "df_url = pd.read_html(url)\n",
    "time.sleep(np.random.randint(100,120)/100)\n",
    "for i in range(0, len(df_url) , 2):\n",
    "    df_url_con = pd.concat([df_url[i], df_url[i+1]], axis=1)\n",
    "    df_url_con[\"上場年\"] = f\"{year}\"\n",
    "    df_result = pd.concat([df_result, df_url_con],ignore_index=True)\n",
    "    df_result = df_result.reset_index(drop=True)\n",
    "df_result.drop(df_result[df_result['企業名'] == \"企業名\"].index , inplace=True)\n",
    "df_result.drop(df_result[df_result['初値'] == \"初値\"].index , inplace=True)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1649765151107,
     "user": {
      "displayName": "樅山輝",
      "userId": "13613188553709245558"
     },
     "user_tz": -540
    },
    "id": "0NxwYzXoo5NZ"
   },
   "outputs": [],
   "source": [
    "\"\"\"ArithmeticErrordf = df.drop_duplicates()\n",
    "#カラムの重複した行を消す。\n",
    "df = df[(df['申込日'] != \"申込日\").values].reset_index(drop=1)\n",
    "#index番号をリセットする。\n",
    "col = ['申込日', '株数', '前週比', '金額', '前週比.1', '株数.1', '前週比.1.1', '金額.1', '前週比.1.2','評価損益率(%)', '信用倍率']\n",
    "df.columns = col\n",
    "#columns（カラムズ）パラメータで列名を自分で指定し修正する。\n",
    "#2重になってたカラムも一つに統一できる。MultiIndex⇒Indexにパラメータを変更。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1649765151600,
     "user": {
      "displayName": "樅山輝",
      "userId": "13613188553709245558"
     },
     "user_tz": -540
    },
    "id": "DcZVZk_MXtgY"
   },
   "outputs": [],
   "source": [
    "# df.to_csv(r\"C:\\Users\\xxp2p\\OneDrive\\ドキュメント\\MEGA_saya\\Traders_web\\margin_transition\\csv\\margin_transition.csv\", encoding='utf-8_sig')\n",
    "#文字化け改善 encoding='utf-8_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#読み込み\n",
    "# pd.read_csv(r\"C:\\Users\\xxp2p\\OneDrive\\ドキュメント\\MEGA_saya\\Traders_web\\margin_transition\\csv\\margin_transition.csv\", header=None, na_values=['-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1649765151603,
     "user": {
      "displayName": "樅山輝",
      "userId": "13613188553709245558"
     },
     "user_tz": -540
    },
    "id": "LpDIiO4dPNEd"
   },
   "outputs": [],
   "source": [
    "#Pythonの文字列を拡張するf,r,b,uについて\n",
    "#f… .format()を使うことなく文字列の中に変数を埋め込むことができる。\n",
    "#r… raw string　通常はバックスラッシュ（改行）があった場合エスケープシーケンスが働くが、raw stringを用いることで無視。\n",
    "#b…バイト列リテラル str型ではなくバイト型のインスタンスを作成します。\n",
    "#u…Unicodeに変換する\n",
    "#fとrだけ使えればOK！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1649765151604,
     "user": {
      "displayName": "樅山輝",
      "userId": "13613188553709245558"
     },
     "user_tz": -540
    },
    "id": "uk6gJjEnUul8"
   },
   "outputs": [],
   "source": [
    "#縦方向に結合する方法|append(), concat()  参考URL https://obgynai.com/python-pandas-index-merge-join/\n",
    "#append：新しい行を追加するメソッド  \n",
    "#concat：columns(列名)や index (行名)を参照して結合するメソッド。ignore_indexとはindexを無視するという意味。concatでデータフレームを結合した際にindexに付与された番号を無視すると言う事になる。\n",
    "#drop dropで行を削除する  参考URL　https://www.sejuku.net/blog/73830"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1649765151605,
     "user": {
      "displayName": "樅山輝",
      "userId": "13613188553709245558"
     },
     "user_tz": -540
    },
    "id": "m2HbwYbx19J6"
   },
   "outputs": [],
   "source": [
    "#データのスクレイピングとデータ整形のコードは分ける。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP9QJ928E/05bykGRGqoQZJ",
   "collapsed_sections": [],
   "name": "traders信用残スクレイピング火曜実行.ipynb",
   "provenance": [
    {
     "file_id": "1eZpjfsHF_ERrNbbJ-uq9B6uDR4SXxpmi",
     "timestamp": 1644795749671
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "0c94bdebe17f03cae7fda76860c6ff1bbe2feabe4a32a304bbf36a54ab3aac08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
