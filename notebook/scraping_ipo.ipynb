{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45049,
     "status": "ok",
     "timestamp": 1649765112153,
     "user": {
      "displayName": "樅山輝",
      "userId": "13613188553709245558"
     },
     "user_tz": -540
    },
    "id": "bTDZnOe4E1sj",
    "outputId": "0a2da34c-1ba5-463f-93bf-d13d0085f8b3"
   },
   "outputs": [],
   "source": [
    "#初期設定\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import lxml\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs4\n",
    "import tqdm\n",
    "import sys\n",
    "import inspect\n",
    "# chromeのwebdriver自動更新用\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://web-kiwami.com/python-beautyfulsoup4.html\n",
    "# http://kondou.com/BS4/\n",
    "# bs4参考"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011 出力中\n",
      "2012 出力中\n",
      "2013 出力中\n",
      "2014 出力中\n",
      "2015 出力中\n",
      "2016 出力中\n",
      "2017 出力中\n",
      "2018 出力中\n",
      "2019 出力中\n",
      "2020 出力中\n",
      "2021 出力中\n",
      "2022 出力中\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "# all_year\n",
    "this_year = 2022 #年変わったら変更\n",
    "get_year = list(range(2011,this_year+1))\n",
    "df_scraping_companies = pd.DataFrame()\n",
    "dict1 = {}\n",
    "for year in get_year:\n",
    "    if 2011 <= year <= 2017:\n",
    "        print(year, \"出力中\")\n",
    "        # 単年のデータ取得2011~2017\n",
    "        url =  rf\"https://www.ipokiso.com/company/{year}.html\"\n",
    "        html_res = requests.get(url)\n",
    "        # ページアクセスエラーの出力\n",
    "        if html_res.status_code != 200:\n",
    "            print(\"requests.getでのurlのアクセスができていません\")\n",
    "            lineno = inspect.currentframe().f_lineno\n",
    "            print(f\"エラーが発生しました。行番号: {lineno}\")\n",
    "            sys.exit()\n",
    "        soup = bs4(html_res.content, 'html.parser')\n",
    "        find_all_list = soup.find_all(href=re.compile(\"company/[0-9]{4}/\"))\n",
    "        url_dict = {\n",
    "            find_all_list[i].text:\"https://www.ipokiso.com/\" + find_all_list[i][\"href\"]\n",
    "            for i in range(len(find_all_list))\n",
    "        }\n",
    "        dict1.update(url_dict)\n",
    "        dfs_list = pd.read_html(url)\n",
    "        time.sleep(np.random.randint(100,120)/100)\n",
    "        for i in range(len(dfs_list)):\n",
    "            dfs_list[i][\"上場年\"] = f\"{year}\"\n",
    "            df_scraping_companies = pd.concat([df_scraping_companies, dfs_list[i]])\n",
    "            df_scraping_companies = df_scraping_companies.reset_index(drop=True)\n",
    "        # df_scraping_companies.drop(df_scraping_companies[df_scraping_companies['企業名'] == \"企業名\"].index , inplace=True)\n",
    "        # df_scraping_companies.drop(df_scraping_companies[df_scraping_companies['初値'] == \"初値\"].index , inplace=True)\n",
    "    else:\n",
    "        # 単年のデータ取得2018~2022\n",
    "        print(year, \"出力中\")\n",
    "        url = rf\"https://www.ipokiso.com/company/{year}.html\"\n",
    "        if year == 2022:\n",
    "            url = r\"https://www.ipokiso.com/company/index.html\"\n",
    "        html_res = requests.get(url)\n",
    "        # ページアクセスエラーの出力\n",
    "        if html_res.status_code != 200:\n",
    "            print(\"requests.getでのurlのアクセスができていません\")\n",
    "            lineno = inspect.currentframe().f_lineno\n",
    "            print(f\"エラーが発生しました。行番号: {lineno}\")\n",
    "            sys.exit()\n",
    "        soup = bs4(html_res.content, 'html.parser')\n",
    "        find_all_list = soup.find_all(href=re.compile(\"company/[0-9]{4}/\"))\n",
    "        url_dict = {\n",
    "            find_all_list[i].text:\"https://www.ipokiso.com/\" + find_all_list[i][\"href\"]\n",
    "            for i in range(len(find_all_list))\n",
    "        }\n",
    "        dict1.update(url_dict)\n",
    "        dfs_list = pd.read_html(url)\n",
    "        # サイト更新時にページのtableのデザインが変更していないかチェック\n",
    "        if year >= 2022:\n",
    "            try:\n",
    "                if dfs_list[0].columns[0] != '企業名':\n",
    "                    raise ValueError(\"銘柄一覧から取得したtableの値が「企業名」ではありません\")\n",
    "                elif dfs_list[1].columns[0] != '総合評価':\n",
    "                    raise ValueError(\"銘柄一覧から取得したtableの値が「総合評価」ではありません\")\n",
    "            except ValueError as e:\n",
    "                # エラーが発生した場合の処理\n",
    "                print(\"Error: {}\".format(e))\n",
    "                lineno = inspect.currentframe().f_lineno\n",
    "                print(f\"エラーが発生した行番号: {lineno}\")   \n",
    "        time.sleep(np.random.randint(100,120)/100)\n",
    "        for i in range(0, len(dfs_list) , 2):\n",
    "            dfs_list_con = pd.concat([dfs_list[i], dfs_list[i+1]], axis=1)\n",
    "            dfs_list_con[\"上場年\"] = f\"{year}\"\n",
    "            if year == 2019:\n",
    "                dfs_list_con = dfs_list_con.rename(columns={'上場 市場': '上場市場'})\n",
    "            if year >= 2020:\n",
    "                dfs_list_con = dfs_list_con.rename(columns={'申し込み期間': '申し込み 期間', '初値上昇率': '初値 上昇率'})\n",
    "            df_scraping_companies = pd.concat([df_scraping_companies, dfs_list_con],ignore_index=True)\n",
    "            df_scraping_companies = df_scraping_companies.reset_index(drop=True)\n",
    "        df_scraping_companies.drop(df_scraping_companies[df_scraping_companies['企業名'] == \"企業名\"].index , inplace=True)\n",
    "        df_scraping_companies.drop(df_scraping_companies[df_scraping_companies['初値'] == \"初値\"].index , inplace=True)\n",
    "    if year != this_year:\n",
    "        time.sleep(np.random.randint(100,120)/100)\n",
    "#codeカラム作成\n",
    "df_scraping_companies_code = df_scraping_companies[\"企業名\"].str.extract(r'([0-9]{4})').fillna(0).astype(int)\n",
    "df_scraping_companies[\"code\"] = df_scraping_companies[\"銘柄 コード\"].fillna(df_scraping_companies_code[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict作成\n",
    "# 空白削除\n",
    "del dict1[\" \"]\n",
    "del dict1[\"\"]\n",
    "del dict1[\"\\n\"]\n",
    "# dictをdf化\n",
    "df_scraping_url = pd.DataFrame.from_dict(dict1, orient='index')\n",
    "# カラム名変更\n",
    "df_scraping_url = df_scraping_url.rename(columns={0:\"url\"})\n",
    "# 間違えているurlを変更\n",
    "df_scraping_url.url[df_scraping_url.url == \"https://www.ipokiso.com/https://www.ipokiso.com/company/2013/zigexn.html\"] = \"https://www.ipokiso.com/company/2013/zigexn.html\"\n",
    "df_scraping_url = df_scraping_url.rename_axis('index').reset_index()\n",
    "df_scraping_url = df_scraping_url.rename(columns={'index':'company_name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraping_url.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "# seleniumでのデータ取得用ループ　評価\n",
    "# chromeのwebdriverのinstallしブラウザを起動する \n",
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(),chrome_options=options)\n",
    "def make_company_info(url):\n",
    "    val_list = []\n",
    "    driver.get(url)\n",
    "    #seleniumの場合ここでsleepしないと正しく取得出来ない場合がある\n",
    "    time.sleep(np.random.randint(100,120)/100)\n",
    "    # HTMLを文字コードをUTF-8に変換してから取得します。\n",
    "    html_res = driver.page_source.encode('utf-8')\n",
    "#ここでシステムエラー処理する？\n",
    "    # if html_res.status_code != 200:\n",
    "    #     print(\"requests.getでのurlのアクセスができていません\")\n",
    "    #     lineno = inspect.currentframe().f_lineno\n",
    "    #     print(f\"エラーが発生しました。行番号: {lineno}\")\n",
    "    #     sys.exit()\n",
    "    soup = bs4(html_res, 'html.parser')\n",
    "    # 全企業codeの取得\n",
    "    try:\n",
    "        code = int(re.search(r\"[0-9]{4}\", soup.title.text).group())\n",
    "    except AttributeError:\n",
    "        f = soup.find_all('h1',text=re.compile(r\"[0-9]{4}\"))[0].text\n",
    "        code = int(re.search(r\"[0-9]{4}\", f).group())\n",
    "    # 3171のtitleのcodeが3172と誤字のための修正\n",
    "    if code == 3172:\n",
    "        f = soup.find_all('h1',text=re.compile(r\"[0-9]{4}\"))[0].text\n",
    "        code = int(re.search(r\"[0-9]{4}\", f).group())\n",
    "    val_list.append(code)\n",
    "    # 成長性等の評価取得\n",
    "    table = soup.find('table',class_=\"company01\")\n",
    "    val = table.find_all('td')\n",
    "    # valからカラムの値に入れたい◎等の値のみ抽出\n",
    "    try:\n",
    "        for i in range(4):\n",
    "            if \">？<\" in str(val[i]):\n",
    "                keyword = \"？\"\n",
    "            elif \"/sannkaku02.gif\" in str(val[i]):\n",
    "                keyword = \"△\"\n",
    "            elif \"/sannkaku.gif\" in str(val[i]):\n",
    "                keyword = \"△\"\n",
    "            elif \"/sankaku.gif\" in str(val[i]):\n",
    "                keyword = \"△\"\n",
    "            elif \"/maru02.gif\" in str(val[i]):\n",
    "                keyword = \"〇\"\n",
    "            elif \"/maru.gif\" in str(val[i]):\n",
    "                keyword = \"〇\"\n",
    "            elif \"/2maru.gif\" in str(val[i]):\n",
    "                keyword = \"◎\"\n",
    "            elif \"/s.gif\" in str(val[i]):\n",
    "                keyword = \"S\"\n",
    "            elif \"/a.gif\" in str(val[i]):\n",
    "                keyword = \"A\"\n",
    "            elif \"/b.gif\" in str(val[i]):\n",
    "                keyword = \"B\"\n",
    "            elif \"/c.gif\" in str(val[i]):\n",
    "                keyword = \"C\"\n",
    "            elif \"/d.gif\" in str(val[i]):\n",
    "                keyword = \"D\"\n",
    "            val_list.append(keyword)\n",
    "    except AttributeError:\n",
    "        val_list.extend([np.nan,np.nan,np.nan,np.nan])\n",
    "        print(f\"成長性listのエラー{code}\")\n",
    "    print(code)\n",
    "    # アンケート調査結果の取得\n",
    "    print(len(soup.find_all(\"div\",class_=\"vote-bar\")))\n",
    "    # 6行の場合\n",
    "    if len(soup.find_all(\"div\",class_=\"vote-bar\")) == 6:\n",
    "        vote_list = []\n",
    "        number_of_votes_list = soup.find_all(\"div\",class_=\"vote-bar\")\n",
    "        for value in number_of_votes_list:\n",
    "            value = int(re.search(r\"\\\"[0-9]+\\\"\",str(value)).group().strip('\"'))\n",
    "            val_list.append(value)\n",
    "    # 5行の場合\n",
    "    elif len(soup.find_all(\"div\",class_=\"vote-bar\")) == 5:\n",
    "        vote_list = []\n",
    "        val_list.append(np.nan)\n",
    "        number_of_votes_list = soup.find_all(\"div\",class_=\"vote-bar\")\n",
    "        for value in number_of_votes_list:\n",
    "            value = int(re.search(r\"\\\"[0-9]+\\\"\",str(value)).group().strip('\"'))\n",
    "            val_list.append(value)\n",
    "    # 10行の場合 5行のアンケートがなぜか２つ表示されている銘柄\n",
    "    elif len(soup.find_all(\"div\",class_=\"vote-bar\")) == 10:\n",
    "        vote_list = []\n",
    "        val_list.append(np.nan)\n",
    "        number_of_votes_list = soup.find_all(\"div\",class_=\"vote-bar\")\n",
    "        number_of_votes_list = number_of_votes_list[0:5]\n",
    "        for value in number_of_votes_list:\n",
    "            value = int(re.search(r\"\\\"[0-9]+\\\"\",str(value)).group().strip('\"'))\n",
    "            val_list.append(value)\n",
    "    # アンケートがない場合\n",
    "    elif len(soup.find_all(\"div\",class_=\"vote-bar\")) == 0:\n",
    "        val_list.extend([np.nan,np.nan,np.nan,np.nan,np.nan,np.nan])\n",
    "    # その他の行数エラー検知\n",
    "    else:\n",
    "        print(\"票の価格帯の数が分岐にありません\")\n",
    "        driver.quit()\n",
    "        raise Exception(\"Error: アンケートの価格帯の数がifの分岐にありません\")\n",
    "    # 今まで取得した情報をリストに格納\n",
    "    evaluation_list.append(val_list)\n",
    "df_scraping_evaluation = df_scraping_url.copy()\n",
    "url_list = df_scraping_url.url.values\n",
    "evaluation_list = []\n",
    "[make_company_info(url) for url in url_list[800:850]] #ここで検証のurl数変更！\n",
    "print(evaluation_list)\n",
    "df_company_01 = pd.DataFrame(evaluation_list,columns=[\n",
    "    \"code\",\n",
    "    \"成長性\",\n",
    "    \"割安性\",\n",
    "    \"話題性\",\n",
    "    \"総合評価\",\n",
    "    \"初値予想_+200%以上\",\n",
    "    \"初値予想_+100%以上+200%未満\",\n",
    "    \"初値予想_+50%以上+100%未満\",\n",
    "    \"初値予想_+20%以上+50%未満\",\n",
    "    \"初値予想_+0%以上+20%未満\",\n",
    "    \"初値予想_+0%未満\",\n",
    "])\n",
    "df_scraping_evaluation[df_company_01.columns] = df_company_01\n",
    "print(df_scraping_evaluation)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "def make_company_info_2(url):\n",
    "    dfs_table_list = pd.read_html(url)\n",
    "    print(dfs_table_list[1].iloc[0,1])\n",
    "    time.sleep(np.random.randint(100,120)/100)\n",
    "    # サイト更新時にデザイン変更がないか検知する（現状ワシントンホテルのみ違うためスルー）\n",
    "    # 回避urlリスト\n",
    "    avoidance_url_list = [\n",
    "        \"https://www.ipokiso.com//company/2019/washingtonhotel.html\",#[0]に１つ余分に優待情報のtableある\n",
    "        \"https://www.ipokiso.com//company/2020/gmo-fg.html\",#table[6][7]が[7][8]にずれている\n",
    "        \"https://www.ipokiso.com//company/2021/frontier.html\",#table[6]が無い\n",
    "        \"https://www.ipokiso.com//company/2021/geolocation.html\",#table[6]が無い\n",
    "        \"https://www.ipokiso.com//company/2015/gmo-media.html\",#table[6][7]が[7][8]にずれている\n",
    "        ]\n",
    "    if url not in avoidance_url_list:\n",
    "        try:\n",
    "            if dfs_table_list[1].iloc[0,0] != \"会社名\":\n",
    "                raise ValueError(\"銘柄一覧から取得したtableの値が「会社名」ではありません\")\n",
    "            elif dfs_table_list[2].iloc[0,0] != '想定価格':\n",
    "                raise ValueError(\"銘柄一覧から取得したtableの値が「想定価格」ではありません\")               \n",
    "            elif dfs_table_list[3].iloc[0,0] != '抽選申込期間':\n",
    "                raise ValueError(\"銘柄一覧から取得したtableの値が「抽選申込期間」ではありません\")\n",
    "            elif not '公募' in dfs_table_list[4].iloc[0,0]:\n",
    "                raise ValueError(\"銘柄一覧から取得したtableの値が「公募株数等」ではありません\")\n",
    "            elif dfs_table_list[5].iloc[0,1] != '証券会社名':\n",
    "                raise ValueError(\"銘柄一覧から取得したtableの値が「証券会社名」ではありません\")\n",
    "        except ValueError as e:\n",
    "            # エラーが発生した場合の処理\n",
    "            print(\"Error: {}\".format(e))\n",
    "            lineno = inspect.currentframe().f_lineno\n",
    "            print(f\"エラーが発生した行番号: {lineno}\")\n",
    "            print(f\"エラーが発生したURL:{url}\")\n",
    "            print(dfs_table_list)\n",
    "            sys.exit()\n",
    "        # 会社名にREITと投資法人という文字列が無いことを判定する\n",
    "        if not (\"REIT\" in dfs_table_list[1].iloc[0,1]) and not (\"投資法人\" in dfs_table_list[1].iloc[0,1]):\n",
    "            try:\n",
    "                if dfs_table_list[6].columns[0] != '株主名':\n",
    "                    raise ValueError(\"銘柄一覧から取得したtableの値が「株主名」ではありません\")\n",
    "                elif not (\"（百万円）\" in dfs_table_list[7].iloc[1,0]) and not (\"（千米ドル）\" in dfs_table_list[7].iloc[1,0]):\n",
    "                    raise ValueError(\"銘柄一覧から取得したtableの値が「財務データ等」ではありません\")\n",
    "            except ValueError as e:\n",
    "                # エラーが発生した場合の処理\n",
    "                print(\"Error: {}\".format(e))\n",
    "                lineno = inspect.currentframe().f_lineno\n",
    "                print(f\"エラーが発生した行番号: {lineno}\")\n",
    "                print(f\"エラーが発生したURL:{url}\")\n",
    "                print(dfs_table_list)\n",
    "                sys.exit()\n",
    "\n",
    "    # ここからdfsの結合、整形コード\n",
    "    df_con_list = []\n",
    "    for i in range(len(dfs_table_list)):\n",
    "        # 基本情報\n",
    "        if i == 1:\n",
    "            # tableの位置指定\n",
    "            df_table = dfs_table_list[i]\n",
    "            # table位置が違うものの例外処理\n",
    "            if df_table.iloc[0,0] != \"会社名\":\n",
    "                df_table = dfs_table_list[i + 1]\n",
    "                print(url)\n",
    "            # df整形：基本情報\n",
    "            cols = df_table.T.values[0]\n",
    "            val = df_table.T.values[1]\n",
    "            # 追加する1件分の基本情報のdf作成\n",
    "            df_table_1 = pd.DataFrame([val],columns=cols)\n",
    "            df_con_list.append(df_table_1)\n",
    "            # 表記の揺れ統一のための修正\n",
    "\n",
    "        # IPO日程と価格決定（初値予想）\n",
    "        if i == 2:\n",
    "            # tableの位置指定\n",
    "            df_table = dfs_table_list[i]\n",
    "            # table位置が違うものの例外処理\n",
    "            if dfs_table_list[2].iloc[0,0] != '想定価格':\n",
    "                df_table = dfs_table_list[i + 1]\n",
    "                print(url)\n",
    "            # df整形：IPO日程と価格決定（初値予想）\n",
    "            cols = df_table.T.values[0]\n",
    "            val = df_table.T.values[1]\n",
    "            df_table_2 = pd.DataFrame([val],columns=cols)\n",
    "            df_con_list.append(df_table_2)\n",
    "            # 表記の揺れ統一のための修正\n",
    "            \n",
    "        # IPOスケジュール\n",
    "        if i == 3:\n",
    "            # tableの位置指定 # tableの位置指定\n",
    "            df_table = dfs_table_list[i]\n",
    "             # table位置が違うものの例外処理\n",
    "            if dfs_table_list[3].iloc[0,0] != '抽選申込期間':\n",
    "                df_table = dfs_table_list[i + 1]\n",
    "                print(url)\n",
    "            \n",
    "            # df整形：IPOスケジュール\n",
    "            cols = df_table.T.values[0]\n",
    "            val = df_table.T.values[1]\n",
    "            df_table_3 = pd.DataFrame([val],columns=cols)\n",
    "            df_con_list.append(df_table_3)\n",
    "            # 表記の揺れ統一のための修正\n",
    "        # IPO当選株数\n",
    "        if i == 4:\n",
    "            # tableの位置指定\n",
    "            df_table = dfs_table_list[i]\n",
    "             # table位置が違うものの例外処理\n",
    "            if not '公募' in dfs_table_list[4].iloc[0,0]:\n",
    "                df_table = dfs_table_list[i + 1]\n",
    "                print(url)\n",
    "            # ワシントンホテルのみ[1]に優待情報があるためtableをずらしている\n",
    "            if url == \"https://www.ipokiso.com//company/2019/washingtonhotel.html\":\n",
    "                df_table = dfs_table_list[i + 1]\n",
    "            \n",
    "            # df整形：IPO当選株数\n",
    "            cols = df_table.T.values[0]\n",
    "            val = df_table.T.values[1]\n",
    "            df_table_4 = pd.DataFrame([val],columns=cols)\n",
    "            df_con_list.append(df_table_4)\n",
    "            # 表記の揺れ統一のための修正\n",
    "\n",
    "        # 幹事証券リスト（管理人独自予想あり）\n",
    "        if i == 5:\n",
    "            # tableの位置指定\n",
    "            df_table = dfs_table_list[i]\n",
    "             # table位置が違うものの例外処理\n",
    "            if dfs_table_list[5].iloc[0,1] != '証券会社名':\n",
    "                df_table = dfs_table_list[i + 1]\n",
    "                print(url)\n",
    "            # df整形：幹事証券リスト（管理人独自予想あり）\n",
    "            df_table_5 = pd.DataFrame()\n",
    "            for j in range(1,len(df_table.iloc[:,0])):\n",
    "                val = df_table.iloc[j,:].values\n",
    "                cols = df_table.iloc[0,:]\n",
    "                cols[0] = \"幹事種類\"\n",
    "                cols = [f\"{col}_{j}\" for col in cols]\n",
    "                df_add = pd.DataFrame([val],columns=cols)\n",
    "                df_table_5 = pd.concat([df_table_5, df_add], axis=1)\n",
    "            df_con_list.append(df_table_5)\n",
    "\n",
    "        # 株主構成、ロックアップなど\n",
    "        if i == 6:\n",
    "            if dfs_table_list[1].iloc[0,0] == \"会社名\":\n",
    "                company_name = dfs_table_list[1].iloc[0,1]\n",
    "            elif dfs_table_list[2].iloc[0,0] == \"会社名\":\n",
    "                company_name = dfs_table_list[2].iloc[0,1]\n",
    "            if not (\"REIT\" in company_name or \"投資法人\" in company_name or  (\"福証\" in company_name and not \"東証\" in company_name)):\n",
    "                # tableの位置指定\n",
    "                df_table = dfs_table_list[i]\n",
    "                # table位置が違うものの例外処理\n",
    "                if dfs_table_list[6].columns[0] != '株主名':\n",
    "                    df_table = dfs_table_list[i + 1]\n",
    "                    print(url)\n",
    "                df_table_6 = pd.DataFrame()\n",
    "                for k in range(0,len(df_table.iloc[:,0])):\n",
    "                    val = df_table.iloc[k,:].values\n",
    "                    cols = df_table.columns\n",
    "                    cols = [f\"{col}_{k+1}\" for col in cols]\n",
    "                    df = pd.DataFrame([val],columns=cols)\n",
    "                    df_table_6 = pd.concat([df_table_6, df], axis=1)\n",
    "                df_con_list.append(df_table_6)\n",
    "\n",
    "            # 企業業績のデータ（5年分）\n",
    "        if i == 7:\n",
    "            if not (\"REIT\" in company_name or \"投資法人\" in company_name):\n",
    "\n",
    "                # tableの位置指定\n",
    "                df_table = dfs_table_list[i]\n",
    "                # table位置が違うものの例外処理\n",
    "                if not (\"（百万円）\" in dfs_table_list[7].iloc[1,0]) and not (\"（千米ドル）\" in dfs_table_list[7].iloc[1,0]):\n",
    "                    df_table = dfs_table_list[i + 1]\n",
    "                    print(url)\n",
    "                \n",
    "                df_table_7 = pd.DataFrame()\n",
    "                for h in range(1,len(df_table.T.iloc[:,0])):\n",
    "                    val = df_table.T.iloc[h,:].values\n",
    "                    # カラム名の誤字修正\n",
    "                    cols = df_table.T.iloc[0,:]\n",
    "                    cols[0] = \"年月\"\n",
    "                    cols = [f\"{col}_{h}年目\" for col in cols]\n",
    "                    df = pd.DataFrame([val],columns=cols)\n",
    "                    df_table_7 = pd.concat([df_table_7, df], axis=1)\n",
    "                df_con_list.append(df_table_7)\n",
    "\n",
    "\n",
    "            # 全件のdfに追加していく\n",
    "    df_table_all = pd.concat(df_con_list,axis=1)\n",
    "    print(url)\n",
    "    print(\"finish\")\n",
    "    return df_table_all\n",
    "df_scraping_table_all = pd.DataFrame()\n",
    "url_list = df_scraping_url.url.values\n",
    "for url in url_list[0:10]:#ここでurl数変更\n",
    "    df_table_all = make_company_info_2(url) \n",
    "    df_scraping_table_all = pd.concat([df_scraping_table_all,df_table_all],axis=0,ignore_index=True)\n",
    "print(df_scraping_table_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 時短のためcode含めたcsv読み込み　コード完成後に削除する\n",
    "# df_scraping_evaluation = pd.read_csv(r\"C:\\Users\\xxp2p\\OneDrive\\デスクトップ\\df_scraping_url.csv\", index_col=0)\n",
    "# df_scraping_evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12/15 dfの作成　4つ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraping_companies.rename(columns={'銘柄 コード': 'code'}, inplace=True)\n",
    "df_scraping_companies[\"code\"] = df_scraping_companies[\"code\"].astype(float)\n",
    "df_scraping_companies.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraping_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraping_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraping_table_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraping_companies[\"code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraping_eva_tab = pd.concat([df_scraping_evaluation,df_scraping_table_all],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraping_eva_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraping_companies[\"code\"].values[300:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraping_eva_tab[\"code\"].values[0:100]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次はここから！2022/12/19予定\n",
    "１、codeの値の型がintやstrで不統一、取得時にintで統一\n",
    "２，df_scraping_companies[\"code\"]は最初からコードと会社名を加工して新カラムcodeをつくる\n",
    "３，df_scraping_eva_tab[\"code\"]はスクレイピング時にint型を指定するように変更する。\n",
    "４，mergeでscraping_allで年がとれるように達成する。\n",
    "５，全てのscrapingの情報から4つのdfの分割等を実施する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraping_all = pd.merge(df_scraping_eva_tab, df_scraping_companies, on=\"code\", how=\"inner\")\n",
    "df_scraping_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価+ 会社設立年 + 市場(G,P,S,…)　+ IPO日程と価格決定(初値予測) + 初値予想アンケート\n",
    "df_assessments = pd.concat([df_scraping_evaluation.loc[:, [\n",
    "    \"code\",\n",
    "    \"成長性\",\n",
    "    \"割安性\",\n",
    "    \"話題性\",\n",
    "    \"総合評価\",\n",
    "    \"初値予想_+200%以上\",\n",
    "    \"初値予想_+100%以上+200%未満\",\n",
    "    \"初値予想_+50%以上+100%未満\",\n",
    "    \"初値予想_+20%以上+50%未満\",\n",
    "    \"初値予想_+0%以上+20%未満\",\n",
    "    \"初値予想_+0%未満\"\n",
    "]],\n",
    "df_scraping_table_all.loc[:, [\n",
    "    \"会社設立\",\n",
    "    \"会社名\",#市場に変形する\n",
    "    \"想定価格\",\n",
    "    \"仮条件\",\n",
    "    \"公募価格\",\n",
    "    \"初値予想（独自）\",\n",
    "    \"初値\"\n",
    "]]],axis=1)\n",
    "df_assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重複チェック\n",
    "df_assessments['code'].value_counts(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assessments['code'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraping_all.to_csv(r\"C:\\Users\\xxp2p\\OneDrive\\デスクトップ\\df_scraping_all.csv\", encoding='utf-8_sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPOスケジュール + IPO当選株数 \n",
    "df_schedule = pd.concat([df_scraping_evaluation.loc[:, [\n",
    "    \"code\",\n",
    "]],\n",
    "df_scraping_table_all.loc[:, [\n",
    "    \"抽選申込期間\",\n",
    "    \"当選発表日\",\n",
    "    \"購入申込期間\",\n",
    "    \"上場日\",\n",
    "    \"公募株数\",\n",
    "    \"売出株数（OA含む）\",\n",
    "    \"当選株数合計\"\n",
    "]]],axis=1)\n",
    "df_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_schedule.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_schedule[df_schedule[\"code\"] == \"3170\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 幹事証券リスト（管理人独自予想あり）\n",
    "df_underwriters = pd.concat([df_scraping_evaluation.loc[:, [\n",
    "    \"code\",\n",
    "]],\n",
    "df_scraping_table_all.filter(regex=\"幹事種類_|証券会社名_|割当率_|割当株数_|当選本数 （枚）_|完全抽選本数 （予想）_\")\n",
    "],axis=1)\n",
    "df_underwriters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_underwriters.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 株主構成 + ロックアップ[6]\n",
    "df_shareholders = pd.concat([df_scraping_evaluation.loc[:, [\n",
    "    \"code\",\n",
    "]],\n",
    "df_scraping_table_all.filter(regex=\"株主名_|比率_|ロック アップ_|ロック  アップ_\")\n",
    "],axis=1)\n",
    "df_shareholders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここ以下で作成　12/13～"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初値予想アンケートの取得実験用　12/13～　これをもとに追加、その後削除する\n",
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(),chrome_options=options)\n",
    "url =  rf\"https://www.ipokiso.com/company/2022/beex.html\"\n",
    "driver.get(url)\n",
    "html_res = driver.page_source.encode('utf-8')\n",
    "soup = bs4(html_res, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アンケート価格リスト\n",
    "soup.find_all(\"p\",class_=\"vote-left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アンケートの票リスト\n",
    "soup.find_all(\"div\",class_=\"vote-bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soup.find_all(\"div\",class_=\"vote-bar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_values_list = soup.find_all(\"p\",class_=\"vote-left\")\n",
    "number_of_votes_list = soup.find_all(\"div\",class_=\"vote-bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes_list = []\n",
    "for value in number_of_votes_list:\n",
    "    # print(value)\n",
    "    value = int(re.search(r\"\\\"[0-9]+\\\"\",str(value)).group().strip('\"'))\n",
    "    votes_list.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraping_table_all.to_csv(r\"C:\\Users\\xxp2p\\OneDrive\\デスクトップ\\df_scraping_table_all.csv\", encoding='utf-8_sig',index=False)\n",
    "\n",
    "df_scraping_table_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これを元に作成2022/12/09~ ↓　全銘柄チェック、その後、例外の銘柄のtableの変更実施\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上本番用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "# csv変換 時短のためこれを読み込む\n",
    "df_code = pd.DataFrame(code_list,columns=[\"code\"])\n",
    "df_scraping_url[df_code.columns] = df_code.values\n",
    "df_scraping_evaluation.to_csv(\"df_scraping_evaluation\", encoding='utf-8_sig')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 時短のためcode含めたcsv読み込み\n",
    "df_scraping_evaluation = pd.read_csv(\"df_scraping_url.csv\", index_col=0)\n",
    "df_scraping_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- IPO当選株数のデータ取得"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup用\n",
    "# requests.getでのデータ取得用ループ　評価\n",
    "def make_company_info(url):\n",
    "    html_res = requests.get(url)\n",
    "    if html_res.status_code != 200:\n",
    "        print(\"requests.getでのurlのアクセスができていません\")\n",
    "        lineno = inspect.currentframe().f_lineno\n",
    "        print(f\"エラーが発生しました。行番号: {lineno}\")\n",
    "        sys.exit()\n",
    "    time.sleep(np.random.randint(100,120)/100)\n",
    "    soup = bs4(html_res.content, 'html.parser')\n",
    "    # 全企業codeの取得\n",
    "    val_list = []\n",
    "    try:\n",
    "        code = re.search(r\"[0-9]{4}\", soup.title.text).group()\n",
    "    except AttributeError:\n",
    "        f = soup.find_all('h1',text=re.compile(r\"[0-9]{4}\"))[0].text\n",
    "        code = re.search(r\"[0-9]{4}\", f).group()\n",
    "    val_list.append(code)\n",
    "    # 成長性等の評価取得\n",
    "    table = soup.find('table',class_=\"company01\")\n",
    "    val = table.find_all('td')\n",
    "    # valからカラムの値に入れたい◎等の値のみ抽出\n",
    "    try:\n",
    "        for i in range(4):\n",
    "            if \">？<\" in str(val[i]):\n",
    "                keyword = \"？\"\n",
    "            elif \"/sannkaku02.gif\" in str(val[i]):\n",
    "                keyword = \"△\"\n",
    "            elif \"/sannkaku.gif\" in str(val[i]):\n",
    "                keyword = \"△\"\n",
    "            elif \"/sankaku.gif\" in str(val[i]):\n",
    "                keyword = \"△\"\n",
    "            elif \"/maru02.gif\" in str(val[i]):\n",
    "                keyword = \"〇\"\n",
    "            elif \"/maru.gif\" in str(val[i]):\n",
    "                keyword = \"〇\"\n",
    "            elif \"/2maru.gif\" in str(val[i]):\n",
    "                keyword = \"◎\"\n",
    "            elif \"/s.gif\" in str(val[i]):\n",
    "                keyword = \"S\"\n",
    "            elif \"/a.gif\" in str(val[i]):\n",
    "                keyword = \"A\"\n",
    "            elif \"/b.gif\" in str(val[i]):\n",
    "                keyword = \"B\"\n",
    "            elif \"/c.gif\" in str(val[i]):\n",
    "                keyword = \"C\"\n",
    "            elif \"/d.gif\" in str(val[i]):\n",
    "                keyword = \"D\"\n",
    "            val_list.append(keyword)\n",
    "    except AttributeError:\n",
    "        val_list = [np.nan,np.nan,np.nan,np.nan]\n",
    "        print(f\"成長性listのエラー{code}\")\n",
    "    evaluation_list.append(val_list)\n",
    "    print(code)\n",
    "df_dict\n",
    "url_list = df_dict.url.values\n",
    "evaluation_list = []\n",
    "[make_company_info(url) for url in url_list[0:3]] #ここで検証のurl数変更！\n",
    "df_company_01 = pd.DataFrame(evaluation_list,columns=[\"code\",\"成長性\",\"割安性\",\"話題性\",\"総合評価\"])\n",
    "df_dict[df_company_01.columns] = df_company_01\n",
    "print(df_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP9QJ928E/05bykGRGqoQZJ",
   "collapsed_sections": [],
   "name": "traders信用残スクレイピング火曜実行.ipynb",
   "provenance": [
    {
     "file_id": "1eZpjfsHF_ERrNbbJ-uq9B6uDR4SXxpmi",
     "timestamp": 1644795749671
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "0c94bdebe17f03cae7fda76860c6ff1bbe2feabe4a32a304bbf36a54ab3aac08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
