{"cells":[{"cell_type":"code","execution_count":845,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45049,"status":"ok","timestamp":1649765112153,"user":{"displayName":"樅山輝","userId":"13613188553709245558"},"user_tz":-540},"id":"bTDZnOe4E1sj","outputId":"0a2da34c-1ba5-463f-93bf-d13d0085f8b3"},"outputs":[],"source":["#初期設定\n","import os\n","import pandas as pd\n","import numpy as np\n","import time\n","import lxml\n","import re\n","import requests\n","from bs4 import BeautifulSoup as bs4\n"]},{"cell_type":"code","execution_count":846,"metadata":{},"outputs":[],"source":["# https://web-kiwami.com/python-beautyfulsoup4.html\n","# http://kondou.com/BS4/\n","# bs4参考"]},{"cell_type":"code","execution_count":847,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2011\n","2012\n","2013\n","2014\n","2015\n","2016\n","2017\n","2018\n","2019\n","2020\n","2021\n","2022\n"]}],"source":["# all_year\n","this_year = 2022 #年変わったら変更\n","get_year = list(range(2011,this_year+1))\n","df_result = pd.DataFrame()\n","dict1 = {}\n","for year in get_year:\n","    if 2011 <= year <= 2017:\n","        print(year)\n","        # 単年のデータ取得2011~2017\n","        url =  rf\"https://www.ipokiso.com/company/{year}.html\"\n","        html_res = requests.get(url)\n","        time.sleep(np.random.randint(100,120)/100)\n","        soup = bs4(html_res.content, 'html.parser')\n","        find_all_list = soup.find_all(href=re.compile(\"company/[0-9]{4}/\"))\n","        url_dict = {\n","            find_all_list[i].text:\"https://www.ipokiso.com/\" + find_all_list[i][\"href\"]\n","            for i in range(len(find_all_list))\n","        }\n","        dict1.update(url_dict)\n","        df_url = pd.read_html(url)\n","        time.sleep(np.random.randint(100,120)/100)\n","        for i in range(len(df_url)):\n","            df_url[i][\"上場年\"] = f\"{year}\"\n","            df_result = pd.concat([df_result, df_url[i]])\n","            df_result = df_result.reset_index(drop=True)\n","        # df_result.drop(df_result[df_result['企業名'] == \"企業名\"].index , inplace=True)\n","        # df_result.drop(df_result[df_result['初値'] == \"初値\"].index , inplace=True)\n","    else:\n","        # 単年のデータ取得2018~2022\n","        print(year)\n","        url = rf\"https://www.ipokiso.com/company/{year}.html\"\n","        if year == 2022:\n","            url = r\"https://www.ipokiso.com/company/index.html\"\n","        html_res = requests.get(url)\n","        time.sleep(np.random.randint(100,120)/100)\n","        soup = bs4(html_res.content, 'html.parser')\n","        find_all_list = soup.find_all(href=re.compile(\"company/[0-9]{4}/\"))\n","        url_dict = {\n","            find_all_list[i].text:\"https://www.ipokiso.com/\" + find_all_list[i][\"href\"]\n","            for i in range(len(find_all_list))\n","        }\n","        dict1.update(url_dict)\n","        df_url = pd.read_html(url)\n","        time.sleep(np.random.randint(100,120)/100)\n","        for i in range(0, len(df_url) , 2):\n","            df_url_con = pd.concat([df_url[i], df_url[i+1]], axis=1)\n","            df_url_con[\"上場年\"] = f\"{year}\"\n","            if year == 2019:\n","                df_url_con = df_url_con.rename(columns={'上場 市場': '上場市場'})\n","            if year >= 2020:\n","                df_url_con = df_url_con.rename(columns={'申し込み期間': '申し込み 期間', '初値上昇率': '初値 上昇率'})\n","            df_result = pd.concat([df_result, df_url_con],ignore_index=True)\n","            df_result = df_result.reset_index(drop=True)\n","        df_result.drop(df_result[df_result['企業名'] == \"企業名\"].index , inplace=True)\n","        df_result.drop(df_result[df_result['初値'] == \"初値\"].index , inplace=True)    "]},{"cell_type":"code","execution_count":848,"metadata":{},"outputs":[],"source":["# dict作成\n","# 空白削除\n","del dict1[\" \"]\n","del dict1[\"\"]\n","del dict1[\"\\n\"]\n","# dictをdf化\n","df_dict = pd.DataFrame.from_dict(dict1, orient='index')\n","# カラム名変更\n","df_dict = df_dict.rename(columns={0:\"url\"})\n","# 間違えているurlを変更\n","df_dict.url[df_dict.url == \"https://www.ipokiso.com/https://www.ipokiso.com/company/2013/zigexn.html\"] = \"https://www.ipokiso.com/company/2013/zigexn.html\""]},{"cell_type":"code","execution_count":844,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>url</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>アイセイ薬局</th>\n","      <td>https://www.ipokiso.com//company/2011/aisei.html</td>\n","    </tr>\n","    <tr>\n","      <th>ミサワ</th>\n","      <td>https://www.ipokiso.com//company/2011/unico.html</td>\n","    </tr>\n","    <tr>\n","      <th>スターフライヤー</th>\n","      <td>https://www.ipokiso.com//company/2011/starflye...</td>\n","    </tr>\n","    <tr>\n","      <th>新田ゼラチン</th>\n","      <td>https://www.ipokiso.com//company/2011/nitta-ge...</td>\n","    </tr>\n","    <tr>\n","      <th>カイオム・\\r\\nバイオサイエンス</th>\n","      <td>https://www.ipokiso.com//company/2011/chiome.html</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>エッジテクノロジー</th>\n","      <td>https://www.ipokiso.com//company/2022/edge-tec...</td>\n","    </tr>\n","    <tr>\n","      <th>ライトワークス</th>\n","      <td>https://www.ipokiso.com//company/2022/lightwor...</td>\n","    </tr>\n","    <tr>\n","      <th>ビッグツリーテクノロジー＆コンサルティング</th>\n","      <td>https://www.ipokiso.com//company/2022/bigtreet...</td>\n","    </tr>\n","    <tr>\n","      <th>セイファート</th>\n","      <td>https://www.ipokiso.com//company/2022/seyfert....</td>\n","    </tr>\n","    <tr>\n","      <th>Recovery International</th>\n","      <td>https://www.ipokiso.com//company/2022/recovery...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1014 rows × 1 columns</p>\n","</div>"],"text/plain":["                                                                      url\n","アイセイ薬局                   https://www.ipokiso.com//company/2011/aisei.html\n","ミサワ                      https://www.ipokiso.com//company/2011/unico.html\n","スターフライヤー                https://www.ipokiso.com//company/2011/starflye...\n","新田ゼラチン                  https://www.ipokiso.com//company/2011/nitta-ge...\n","カイオム・\\r\\nバイオサイエンス       https://www.ipokiso.com//company/2011/chiome.html\n","...                                                                   ...\n","エッジテクノロジー               https://www.ipokiso.com//company/2022/edge-tec...\n","ライトワークス                 https://www.ipokiso.com//company/2022/lightwor...\n","ビッグツリーテクノロジー＆コンサルティング   https://www.ipokiso.com//company/2022/bigtreet...\n","セイファート                  https://www.ipokiso.com//company/2022/seyfert....\n","Recovery International  https://www.ipokiso.com//company/2022/recovery...\n","\n","[1014 rows x 1 columns]"]},"execution_count":844,"metadata":{},"output_type":"execute_result"}],"source":["df_dict"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# code_listの作成（詳細情報の取得ループ）全てのcode取得ok！\n","def make_company_info(url):\n","    html_res = requests.get(url)\n","    time.sleep(np.random.randint(100,120)/100)\n","    soup = bs4(html_res.content, 'html.parser')\n","    try:\n","        code = re.search(r\"[0-9]{4}\", soup.title.text).group()\n","    except AttributeError:\n","        f = soup.find_all('h1',text=re.compile(r\"[0-9]{4}\"))[0].text\n","        code = re.search(r\"[0-9]{4}\", f).group()\n","    code_list.append(code)\n","    print(url)\n","    print(code)\n","code_list = []\n","url_list = df_dict.url.values\n","[make_company_info(url) for url in url_list]\n","print(code_list)\n"]},{"cell_type":"markdown","metadata":{},"source":["以後チェック用code"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# URLdict（単年）作成 確認用\n","year = \"2020\"\n","url = f'https://www.ipokiso.com/company/{year}.html'\n","html_res = requests.get(url)\n","time.sleep(np.random.randint(100,120)/100)\n","soup = bs4(html_res.content, 'html.parser')\n","find_all_list = soup.find_all(href=re.compile(f\"company/{year}/\"))\n","url_dict = {\n","    find_all_list[i].text:\"https://www.ipokiso.com/\" + soup.find_all(href=re.compile(f\"company/{year}/\"))[i][\"href\"]\n","    for i in range(len(find_all_list))\n","}\n","url_dict"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 単年のデータ取得2011~2017　確認用\n","df_result = pd.DataFrame()\n","year = \"2011\"\n","url =  rf\"https://www.ipokiso.com/company/{year}.html\"\n","html_res = requests.get(url)\n","time.sleep(np.random.randint(100,120)/100)\n","soup = bs4(html_res.content, 'html.parser')\n","find_all_list = soup.find_all(href=re.compile(f\"company/{year}/\"))\n","print(len(find_all_list))\n","df_url = pd.read_html(url)\n","time.sleep(np.random.randint(100,120)/100)\n","for i in range(len(df_url)):\n","    df_url[i][\"上場年\"] = f\"{year}\"\n","    df_result = pd.concat([df_result, df_url[i]])\n","    df_result = df_result.reset_index(drop=True)\n","df_result.drop(df_result[df_result['企業名'] == \"企業名\"].index , inplace=True)\n","df_result.drop(df_result[df_result['初値'] == \"初値\"].index , inplace=True)\n","df_result"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 単年のデータ取得2018~2022　確認用\n","df_result = pd.DataFrame()\n","year = \"2022\"\n","url =  rf\"https://www.ipokiso.com/company/{year}.html\"\n","if year == \"2022\":\n","    url =  r\"https://www.ipokiso.com/company/index.html\"\n","html_res = requests.get(url)\n","time.sleep(np.random.randint(100,120)/100)\n","soup = bs4(html_res.content, 'html.parser')\n","find_all_list = soup.find_all(href=re.compile(f\"company/{year}/\"))\n","print(len(find_all_list))\n","df_url = pd.read_html(url)\n","time.sleep(np.random.randint(100,120)/100)\n","for i in range(0, len(df_url) , 2):\n","    df_url_con = pd.concat([df_url[i], df_url[i+1]], axis=1)\n","    df_url_con[\"上場年\"] = f\"{year}\"\n","    df_result = pd.concat([df_result, df_url_con],ignore_index=True)\n","    df_result = df_result.reset_index(drop=True)\n","df_result.drop(df_result[df_result['企業名'] == \"企業名\"].index , inplace=True)\n","df_result.drop(df_result[df_result['初値'] == \"初値\"].index , inplace=True)\n","df_result"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1649765151107,"user":{"displayName":"樅山輝","userId":"13613188553709245558"},"user_tz":-540},"id":"0NxwYzXoo5NZ"},"outputs":[],"source":["\"\"\"ArithmeticErrordf = df.drop_duplicates()\n","#カラムの重複した行を消す。\n","df = df[(df['申込日'] != \"申込日\").values].reset_index(drop=1)\n","#index番号をリセットする。\n","col = ['申込日', '株数', '前週比', '金額', '前週比.1', '株数.1', '前週比.1.1', '金額.1', '前週比.1.2','評価損益率(%)', '信用倍率']\n","df.columns = col\n","#columns（カラムズ）パラメータで列名を自分で指定し修正する。\n","#2重になってたカラムも一つに統一できる。MultiIndex⇒Indexにパラメータを変更。\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":500,"status":"ok","timestamp":1649765151600,"user":{"displayName":"樅山輝","userId":"13613188553709245558"},"user_tz":-540},"id":"DcZVZk_MXtgY"},"outputs":[],"source":["# df.to_csv(r\"C:\\Users\\xxp2p\\OneDrive\\ドキュメント\\MEGA_saya\\Traders_web\\margin_transition\\csv\\margin_transition.csv\", encoding='utf-8_sig')\n","#文字化け改善 encoding='utf-8_sig"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#読み込み\n","# pd.read_csv(r\"C:\\Users\\xxp2p\\OneDrive\\ドキュメント\\MEGA_saya\\Traders_web\\margin_transition\\csv\\margin_transition.csv\", header=None, na_values=['-'])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1649765151603,"user":{"displayName":"樅山輝","userId":"13613188553709245558"},"user_tz":-540},"id":"LpDIiO4dPNEd"},"outputs":[],"source":["#Pythonの文字列を拡張するf,r,b,uについて\n","#f… .format()を使うことなく文字列の中に変数を埋め込むことができる。\n","#r… raw string　通常はバックスラッシュ（改行）があった場合エスケープシーケンスが働くが、raw stringを用いることで無視。\n","#b…バイト列リテラル str型ではなくバイト型のインスタンスを作成します。\n","#u…Unicodeに変換する\n","#fとrだけ使えればOK！"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1649765151604,"user":{"displayName":"樅山輝","userId":"13613188553709245558"},"user_tz":-540},"id":"uk6gJjEnUul8"},"outputs":[],"source":["#縦方向に結合する方法|append(), concat()  参考URL https://obgynai.com/python-pandas-index-merge-join/\n","#append：新しい行を追加するメソッド  \n","#concat：columns(列名)や index (行名)を参照して結合するメソッド。ignore_indexとはindexを無視するという意味。concatでデータフレームを結合した際にindexに付与された番号を無視すると言う事になる。\n","#drop dropで行を削除する  参考URL　https://www.sejuku.net/blog/73830"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1649765151605,"user":{"displayName":"樅山輝","userId":"13613188553709245558"},"user_tz":-540},"id":"m2HbwYbx19J6"},"outputs":[],"source":["#データのスクレイピングとデータ整形のコードは分ける。"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyP9QJ928E/05bykGRGqoQZJ","collapsed_sections":[],"name":"traders信用残スクレイピング火曜実行.ipynb","provenance":[{"file_id":"1eZpjfsHF_ERrNbbJ-uq9B6uDR4SXxpmi","timestamp":1644795749671}]},"kernelspec":{"display_name":"yt_38","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"vscode":{"interpreter":{"hash":"0c94bdebe17f03cae7fda76860c6ff1bbe2feabe4a32a304bbf36a54ab3aac08"}}},"nbformat":4,"nbformat_minor":0}
